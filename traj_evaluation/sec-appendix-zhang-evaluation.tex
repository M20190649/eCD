\vspace{-1ex}
\section*{Appendix C: Zhang's Evaluation}

\stitle{\textcolor{blue}{Advantages of Zhang' Evaluation\cite{Zhang:Evaluation}}}.

(1) Tests a wild list of algorithms

(2) Tests the average errors of algorithms from the view of application: Spatio-temporal query

\stitle{\textcolor{blue}{Limitations of Zhang' Evaluation\cite{Zhang:Evaluation}}}.

(1) Algorithms are listed without organizing.

(2) Many algorithms are not so important or typical or practical for trajectory simplification, \eg Uniform is not error bounded, Dead Reckoning has really poor compression ratios.
% It is easy to conclude these from the natures of these algorithms or from preview works,

(3) The optimal algorithms of \ped and \sed are not covered in the work.

(4) Some important sub-optimal algorithms are not covered, \eg sleeve, cised.

(5) Distance metrics, especially \ped and \sed, are not compared in the work.

(6) Compression ratio is not tested and compared, though it is one of the most important metrics of trajectory simplification algorithms.

(7) Another metric, runtime time, is not systematically tested in the same environment.

(8) The max distance errors are not shown in the tests, and it is unjustified to take the average errors as the only quality metric of algorithms and neglect the max distance errors, especially that some algorithms in the tests are not error bounded or use threshold on the accumulate errors.
Indeed, the average errors and the max errors both are metrics of quality of algorithms. Some algorithms, \eg the optimal algorithms, may have relative larger average errors, however, they are error bounded. Some algorithms, \eg algorithms MRPA and DOTS that apply the accumulate SED error, ISSD, may have very large max errors though they have relative small average errors.

(9) It is unfair to conclude the guidelines based on the average error only. As we know, the algorithm that outputs the original input points is sure having the best/smallest average errors, but it is nonsense. In other words, algorithm has relative large average error as well as has good compression ratios may be a good choice in some scenarios.
