
\textcolor{blue}{\section{Aging of trajectories}}

Suppose we first compressed a trajectory $\dddot{\mathcal{T}_0}$ using an \lsa algorithm $\mathcal{A}$ with error bound $\epsilon_1$ to $\overline{\mathcal{T}}_1$, then as time progresses, we need a coarser trajectory $\overline{\mathcal{T}}_2$ derived from $\overline{\mathcal{T}}_1$. 
What is the relationship among $\overline{\mathcal{T}}_2$, $\overline{\mathcal{T}}_1$ and $\dddot{\mathcal{T}_0}$? And what is the right way to get the coarser trajectory $\overline{\mathcal{T}}_2$?
%
This section is to answer these questions from the views of friendliness which was defined in \cite{Cao:Spatio} but seldom discussed in other works.
Note that if we get $\overline{\mathcal{T}}_1$ and $\overline{\mathcal{T}}_2$ by optimal algorithms \wrt error bounds $\epsilon_1$ and $\epsilon_2$ in turn, then $\overline{\mathcal{T}}_2$ may not be optimal of $\dddot{\mathcal{T}_0}$ \wrt any error bound \cite{Cao:Spatio}. Thus, in this section, we let alone the optimal algorithms.

%\subsection{Friendliness of Data Aging}
%We discuss the relationship between $\overline{\mathcal{T}}_1$ and $\overline{\mathcal{T}}_2$ from a view of friendliness which was defined in \cite{Cao:Spatio} but seldom discussed in other works.
	
\stitle{Aging friendly \cite{Cao:Spatio}}. {A \lsa algorithm $\mathcal{A}$ is aging friendly with respect to a distance metric $\mathcal{D}$ if for every $\epsilon_1$ and every $\epsilon_2$ such that $\epsilon_1 < \epsilon_2$, and for every trajectory $\dddot{\mathcal{T}}$, $\mathcal{A}(\dddot{\mathcal{T}}, \epsilon_2, \mathcal{D})= \mathcal{A}(\mathcal{A}(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D})$.}

Cao et al. also proved in \cite{Cao:Spatio} that ``the top-down algorithm \dpa is aging friendly \wrt~\ped and \sed" on the premise that the second run of algorithm \dpa takes as input the whole simplified trajectory produced by the first run. 
%, including the same start and end points of trajectories. 
We then discuss the error between $\overline{\mathcal{T}}_2$ and $\dddot{\mathcal{T}_0}$.

\begin{theorem}
	\label{theo-aging-dp}
	Given error bounds $\epsilon_1>0$ and $\epsilon_2>0$, for every distance metric $\mathcal{D}$ of \ped and \sed, the error bound between original trajectory $\dddot{\mathcal{T}}$ and simplified trajectory $\overline{\mathcal{T}}=DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D})$ is $max\{\epsilon_1, \epsilon_2\}$.
\end{theorem}

\begin{proof}
	For $\epsilon_2 \ge \epsilon_1$, as proved in \cite{Cao:Spatio}, $DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}) = DP(\dddot{\mathcal{T}}, \epsilon_2, \mathcal{D})$, which has the max error of $\epsilon_2$ to the original trajectory $\dddot{\mathcal{T}}$.
	
	For $\epsilon_1 > \epsilon_2$, we will first prove $DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}) = DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D})$ by total induction on the number of points of \trajec{T}.
	
	(1)  For a trajectory \trajec{T} with one or two points ($n=1$ or $n=2$), the simplified trajectories with any $\epsilon$ are sure identical to the original trajectory. 
	Consider a trajectory \trajec{T} =	$[P_0, P_1, P_2]$ ($n = 3$),
	if the distance from $P_1$ to $\overline{P_0P_2}$ is less than $\epsilon_1$, then $DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}) = [P_0, P_2]$. Obviously $DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}))$ is $[P_0, P_2]$ too;	 
	if the distance from $P_1$ to $\overline{P_0P_2}$ is larger than $\epsilon_1$, then $DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D})=\dddot{\mathcal{T}}$, and $DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}) = DP(\dddot{\mathcal{T}}, \epsilon_2, \mathcal{D})=\dddot{\mathcal{T}}$.
	
	(2) Assume it is true for every trajectory \trajec{T} having $n$, $(n \ge 3)$, points.
	Consider a trajectory with $n+1$ points. Let $d_{max}$ denote the maximum distance between point $P_i$, $i \in [0,n]$, and the line segment $\overline{P_0P_{n}}$. 
	If $d_{max}<\epsilon_1$, then $DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D})=[P_0, P_{n}]$, and $DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}) = DP([P_0, P_{n}], \epsilon_2, \mathcal{D})=[P_0, P_{n}]$.
	If $d_{max} > \epsilon_1$, then in $DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D})$, point $P_i$ will split the trajectory \trajec{T} into two sub-trajectories, \ie $[P_0, ..., P_i]$ and $[P_{i}, ..., P_{n}]$, and continue to simplify each sub-trajectories. Hence, the result of $DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D})$ is the union of $DP([P_0, ..., P_i], \epsilon_1, \mathcal{D})$ and $DP([P_i, ..., P_n], \epsilon_1, \mathcal{D})$.
	Obviously, the points $P_0$, $P_i$ and $P_n$ are in the simplified trajectory of $DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D})$, and $P_i$ is still the first spiting point of the \dpa algorithm taking the simplified trajectory and $\epsilon_2$ as input. Hence, $DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D})$ is the union of $DP(DP([P_0, ..., P_i], \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D})$ and $DP(DP([P_i, ..., P_n], \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D})$. By the assumption, we have $DP(DP([P_0, ..., P_i], \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}) = DP([P_0, ..., P_i], \epsilon_1, \mathcal{D})$ and $DP(DP([P_i, ..., P_n], \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}) = DP([P_i, ..., P_n], \epsilon_1, \mathcal{D})$. Thus, $DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}) = DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D})$.
	
	Combining (1) and (2), we have $DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D}) = DP(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D})$, whose max error to the original trajectory $\dddot{\mathcal{T}}$ is $\epsilon_1$.
\end{proof}

%From the above we can find that the {aging friendliness} of algorithm \dpa using \ped or \sed is related to the {batch and top-down nature} of \dpa.
% \ie suppose in the first run, it splits $\dddot{\mathcal{T}_0}$ into two sub-trajectories by point $P_i$, then $P_i$ is saved in $\overline{\mathcal{T}}_1$, and it is the first splitting point in the second run taking $\overline{\mathcal{T}}_1$ as input. 
%However, when using \dad, the point $P_i$ may be not the splitting point in the second run as \dad is the direction deviation between two line segments and ...


\begin{theorem}
	\label{theo-aging-dp-dad}
	The top-down algorithm \dpa is not aging friendly \wrt~\dad, and given error bounds $\epsilon_1>0$ and $\epsilon_2>0$, the error bound between original trajectory $\dddot{\mathcal{T}}$ and simplified trajectory $\overline{\mathcal{T}}=DP(DP(\dddot{\mathcal{T}}, \epsilon_1, \dad), \epsilon_2, \dad)$ is $\epsilon_1 + \epsilon_2$.
\end{theorem}

\begin{proof}
	\todo{}
\end{proof}



We next discuss the aging friendliness of other algorithms.

\begin{figure}
	\centering
	\includegraphics[scale=0.66]{Figures/Fig-aging-pavlidis.png}

	\caption{\small A counter example of the aging friendliness of algorithm \tpa, where (1) the original trajectory is compressed using $\epsilon_2=6$ to three points $\{P_0, P_3, P_4\}$; (2) the original trajectory is first compressed using $\epsilon_1=3$ to four points $\{P_0, P_1, P_3, P_4\}$, then compressed using $\epsilon_2=6$ to two points $\{P_0, P_4\}$. }
	\vspace{-1ex}
	\label{fig:aging-pavlidis}
\end{figure}


\begin{theorem}
\label{theo-aging-tp}
The bottom-up algorithm \tpa is not aging friendly \wrt~\ped, \sed or \dad.
\end{theorem}

\begin{proof}
We will prove the theorem by construction.
As shown in Figure~\ref{fig:aging-pavlidis}, we let error bounds $\epsilon_1 =3$ and $\epsilon_2=6$, and without losing generality, we use \ped as the distance metric.

\underline{(1) ${\tpa}(\dddot{\mathcal{T}}, 6, \ped)$}. In the first step, it merges $\overline{P_1P_2}$ and $\overline{P_2P_3}$ to $\overline{P_1P_3}$ as the merging of them has the lowest cost of $2$, the distance from point $P_2$ to line segment $\overline{P_1P_3}$; then it merges $\overline{P_0P_1}$ and $\overline{P_1P_3}$ to $\overline{P_0P_3}$ with the lowest cost of $4.5$, the distance from point $P_1$ to line segment $\overline{P_0P_3}$; at last, because the merging of $\overline{P_0P_3}$ and $\overline{P_3P_4}$ has a cost of $7$, the distance from point $P_2$ to line segment $\overline{P_0P_4}$, which is larger than the error bound of $6$, it outputs three points $\{P_0, P_3, P_4\}$.

\underline{(2) ${\tpa}(\tpa(\dddot{\mathcal{T}}, 3, \ped), 6, \ped)$}. In the first round ($\epsilon_1=3$), the original trajectory is compressed to four points $\{P_0, P_1, P_3, P_4\}$, and in the second round ($\epsilon_1=6$), because all points in the result trajectory $\{P_0, P_1, P_3, P_4\}$ have distances to line segment $P_0P_4$ less than $6$, it is finally compressed to two points $\{P_0, P_4\}$.

In this case, ${\tpa}(\dddot{\mathcal{T}}, 6, \ped) \ne {\tpa}(\tpa(\dddot{\mathcal{T}}, 3, \ped), 6, \ped)$. Thus, the \tpa algorithm is not aging friendly \wrt~\ped.
\textcolor{blue}{Similarly, we can prove that \tpa is not aging friendly \wrt~ \sed or \dad. Hence, we have the conclusion.}
\end{proof}

\begin{lemma}
	\label{theo-aging-tp}
	The online algorithm \squishe is not aging friendly \wrt~\sed.
\end{lemma}

\begin{proof}
	\textcolor{blue}{(In brief) The \squishe algorithm runs in a bottom-up manner that is a slight variation of algorithm \tpa. As \tpa, it is not aging friendly \wrt~\sed.}
\end{proof}

	
\begin{figure}[tb!]
	\centering
	\includegraphics[scale=0.66]{Figures/Fig-aging-incre.png}
	\vspace{-1ex}
	\caption{\small Counter examples of the aging friendliness of incremental algorithms (either online or one-pass).}
	\vspace{-1ex}
	\label{fig:aging-incre}
\end{figure}

\begin{theorem}
	\label{theo-aging-online}
	The online and one-pass algorithms are not aging friendly \wrt~\ped, \sed or \dad.

\end{theorem}

\begin{proof}
	For online algorithm \squishe, it only supports \sed and it is not aging friendly \wrt~\sed.
	For other online and one-pass algorithms, though they apply different distance checking approaches, they run in a common incremental manner, \ie they incrementally read data points until they can not represent those read points by one line segment, then they output the simplified sub-trajectory and continue to process the rest data points. We next construct counter examples to show that an incremental algorithm $\mathcal{A}$ is not aging friendly.
	
	\underline{(1) ${\mathcal{A}}(\dddot{\mathcal{T}}, \epsilon_2, \mathcal{D})$}. As shown in Figure~\ref{fig:aging-incre}-(1)(3)(5), the algorithm $\mathcal{A}$ incrementally reads $\{P_0, P_1,\dddot, P_5\}$ and finds they can be represented by line segment $\overline{P_0P_4}$, thus the process progresses. Then, after point $P_6$ is read, it finds that these points can not be represented by any line segment, hence $\overline{P_0P_5}$ is output. Finally, the algorithm outputs $\{P_0, P_5, P_6\}$.
	
	\underline{(2) ${\mathcal{A}}(\mathcal{A}(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D})$}. When using $\epsilon_1=4$ or $\epsilon_1=30^o $, the algorithm also outputs $\{P_0, P_5, P_6\}$. Then $\{P_0, P_5, P_6\}$ is compressed using $\epsilon_2=6$ or $\epsilon_2=45^o$ to $\{P_0, P_6\}$, as shown in Figure~\ref{fig:aging-incre}-(2)(4)(6).
	
	Combining (1) and (2), it is clear that the incremental algorithms are not aging friendly \wrt~distance metric \ped, \sed or \dad.
\end{proof}

\todo{From the above we can find that only algorithm \dpa is aging friendly, others are not. Indeed, {aging friendly} is the result of the {specific nature}, \ie batch and top-down, of algorithm \dpa.}
% pretty and rare feature of






\begin{theorem}
	\label{theo-aging-distance}
	Given error bounds $\epsilon_1>0$ and $\epsilon_2>0$, for every error bounded and non-optimal \lsa algorithm $\mathcal{A}$ other than \dpa and for every distance metric $\mathcal{D}$ of \ped, \sed and \dad, the error bound between original trajectory $\dddot{\mathcal{T}}$ and simplified trajectory $\overline{\mathcal{T}}=\mathcal{A}(\mathcal{A}(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D})$ is $\epsilon_1+ \epsilon_2$.
\end{theorem}

\begin{proof} 	
	(1) We first prove that the  error bound between them is not less than $\epsilon_1+ \epsilon_2$.
	If $\mathcal{A}$ is a bottom-up algorithm, then from Figure~\ref{fig:aging-pavlidis}-(2) we can find that the error bound between original trajectory $\dddot{\mathcal{T}}$ and simplified trajectory $\overline{\mathcal{T}}=\mathcal{A}(\mathcal{A}(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{D}), \epsilon_2, \mathcal{D})$ is not less than $\epsilon_1+ \epsilon_2$.
	If $\mathcal{A}$ is an incremental algorithm, either online or one-pass, then from Figure~\ref{fig:aging-incre} we also have the conclusion.
	
	(2) We next prove that the error bound between them is not more than $\epsilon_1+ \epsilon_2$. Suppose a point $P_k$ is represented by line segment $\overline{P_iP_j}$ with error bound $\epsilon_1$, and points $P_i$ and $P_j$ are further represented by line segment $\overline{P_sP_t}$ with error bound $\epsilon_2$ (Figure~\ref{fig:aging-error}).
	If the distance metric is \ped, then the distance from $P'_k$ to $\overline{P_sP_t}$ is less than $\epsilon_2$, hence, the distance from $P_k$ to $\overline{P_sP_t}$ is less than $\epsilon_1 + \epsilon_2$.
	If it is \sed, then $|P_iP''_i|<\epsilon_2$, $|P_jP''_j|<\epsilon_2$, and $\frac{|P_iP'_k|}{|P'_kP_j|} = \frac{|P''_iP''_k|}{|P''_kP''_j|}$, hence, $|P'_kP''_k|<\epsilon_2$, and the distance from $P_k$ to $\overline{P_sP_t}$, \ie $|P_kP''_k|$, is less than $\epsilon_1 + \epsilon_2$.
	If it is \dad, then obviously the error between $\overline{P_kP_{k+1}}$ and $\overline{P_sP_t}$ is not more than $\epsilon_1+ \epsilon_2$.
	

	
	Combining (1) and (2), we have the conclusion.
\end{proof}

\begin{figure}[tb!]
	\centering
	\includegraphics[scale=0.6]{Figures/Fig-aging-error.png}
	
	\caption{\small Examples of aging error.}
	\vspace{-2ex}
	\label{fig:aging-error}
\end{figure}