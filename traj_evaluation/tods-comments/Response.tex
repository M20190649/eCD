\documentclass{letter}
\usepackage{geometry}

% duan
\usepackage{xspace}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{cite}

\newcommand{\marked}[1]{\textcolor{red}{#1}}

\newcommand{\kw}[1]{{\ensuremath {\mathsf{#1}}}\xspace}

\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}
\newcommand{\kwlog}{\emph{w.l.o.g.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\sstab}{\rule{0pt}{8pt}\\[-2.4ex]}

\newcommand{\topk}[1]{\kw{top}--\kw{#1}}
\newcommand{\topdown}{\kw{topDown}}
\newcommand{\extsubgraph}{\kw{compADS^+}}
\newcommand{\drfds}{\kw{FIDES^+}}
\newcommand{\extsubgraphold}{\kw{compADS}}
\newcommand{\findtimax}{\kw{maxTInterval}}
\newcommand{\findtimin}{\kw{minTInterval}}
\newcommand{\meden}{\kw{MEDEN}}

\newcommand{\tranformgraph}{\kw{convertAG}}
\newcommand{\mergecc}{\kw{strongMerging}}
\newcommand{\strongpruning}{\kw{strongPruning}}
\newcommand{\boundedprobing}{\kw{boundedProbing}}

\newcommand{\AFPR}{\kw{AFP}-\kw{reduction}}
\newcommand{\nwm}{{\sc nwm}\xspace}


\newcommand{\cone}[1]{{$\mathcal{C}{#1}$}}
\renewcommand{\circle}[1]{{$\mathcal{O}{#1}$}}
\newcommand{\pcircle}[1]{{$\mathcal{O}^c{#1}$}}

\newcommand{\vv}{\overrightarrow}


\newcommand{\todo}[1]{\textcolor{red}{Todo...#1}}
\begin{document}



Prof. {Chris Jermaine} \\
Editor-in-Chief		\\
ACM TODS	\\



Dear Prof. Jermaine,

Attached please find a revised version of our submission to
the TODS, \emph{Error Bounded Line Simplification Algorithms for Trajectory Compression: An Experimental Evaluation}.


\textbf{[Comments from Editors]}.\emph{These reviews, by recognized experts in the field, have obviously been prepared with care. Based on the reviews, the recommendation by handling Associate Editor Dr. Seeger, and my own assessment, I find that the paper needs to undergo a successful major revision to be acceptable for publication in TODS. As Dr. Seeger wrote to me, the reviews are quite consistent: two of them ask for a major revision, one for a minor revision. All of them agree that the survey paper is well written. Reviewer 1 rises most concerns regarding the variety of methods considered (the paper does not consider techniques where new points are used for compression), not considering map matching as a preprocessing step, error metrics, impact on applications, and the small data sets used in the experiments. The authors should carefully consider these points.}


\textcolor{red}{The paper has been substantially revised according to the comments of you, Dr. Seeger and referees. In particular, we have (a) added techniques that new points are used for compression, (b) impacts of error metrics on applications, and (c) a larger data set of UCar is used in the experiments.}

\textcolor{red}{Note that map matching is still not considered as a preprocessing step because (1) a moving object is not limited in urban, e.g., Geolife..., Mopsi..., so, many trajectory (or part of the trajectory) could not match to a map, and (2) a trajectory is not necessary matched to roads before compression, indeed, they are two independent works, and there are two ways to combine trajectory simplification and map-matching, \ie simplified then map-matching [ref], and in the verse, map-matching then simplified [ref]. Maybe it is a valuable work to compare these two ways, however, it is outside the scope of the work.}


We would like to thank all the referees for their thorough reading of our paper and for their valuable comments.

Below please find our detailed responses to the comments.



%******************* reviewer 1 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 1.}

\line(1,0){100}


\textbf{[R1C1]} \emph{What is ``min-\# problem" mentioned in Section 1 (second paragraph)?}

The \emph{$min-\#$ problem} of trajectory simplification is, given a curve (\ie an original trajectory) and an $\epsilon$, to construct an approximate curve with error within $\epsilon$ and having the minimum number of line segments.

We have explained this in Section 1 (second paragraph). Thanks for pointing out this!

\textbf{[R1C2]} \emph{It is stated in the paper that "The idea of piece-wise line simplification (LS) comes from computational geometry, whose target is to approximate a fine piece-wise linear curve with a coarse one (whose corresponding data points are a subset of the original one), such that the maximum distance of the former to the latter is bounded by a user specified threshold." This is actually misleading. Line simplification actually could be performed by using points other than original ones to represent the original line. The authors did mention those algorithms which consider points outside the original trajectories in the appendix. However, to make sure the correctness of the statements, it's better to mention that in the beginning of the paper. In addition, it is necessary to explain why this paper only considers the line simplification algorithms that consider the original points only. }

We have fixed this bug and added ``weak simplification" algorithms that allow insert new data points into the output trajectory. Please refer to Section 1 (second paragraph) and Section 6 for more details.
Thanks!

\textbf{[R1C3]} \emph{ For trajectories that capture the moving objects' movement along the road network, the points are normally mapped to road networks. In other words, it is not necessary to use piece-wise line representation. It looks like the authors have not considered this case, which is actually one of the most common cases. Can the authors please explain why map-matching step is not considered even for trajectories that capture the movements in road networks? }


We do not consider map-matching as the pre-process of trajectory simplification for some serious considerations:

(1) First of all, \emph{any original trajectory is naturally a sequence of piece-wise line segments}, and from the point of views of data management and data analysis, saving a trajectory as a sequence of data points (either original or simplified) is a lighter, easier and more efficient way than saving the map-matched data. Thus, \emph{map-matching is not necessary the pre-process of trajectory simplification.} 
%On the contrary, presenting a trajectory on roads needs extra information (road network) and extra heavy computation (map-matching).


(2) Moving objects often move in areas not on roads or without fine road network. For example, human being, wild animals, bikes, air plane may move on ground, at gardens, in countryside and in the atmosphere, where no fine road information is available. In these cases, \emph{map-matching should not be the pre-step of trajectory simplification.}

(3) In urban having road network, of cause, the data points could be mapped to road networks. However, this is not a work must be done. In some server-side applications like moving object monitoring, it just shows the piece-wise line trajectory on a map without map-matching in the consideration of efficiency. Besides, map-matching is most likely an application-level task called by application such as navigation. As we known, trajectory simplification as a fundamental functionality should (better) not depend on these high-level applications. Thus, \emph{map-matching should not be the pre-step of trajectory simplification too.}
%
We also know that there is a kind of trajectory compression called road-aware trajectory compression [r1, r2] that either first maps a trajectory to road network then simplifies those mapped data points by \emph{numerical streams} method[r3], such as PRESS [r1], or first simplifies the trajectory by some line simplification method then maps the result to roads, such as the Dilution-matching-encoding method [r2]. In the first case, there is no line simplification, and in the second case, line simplification is before, and hence independence of, map-matching. 

(4) Let map-matching be the pre-step will require extra information (road network) and extra heavy computation (map-matching), and introduce extra error and running time, which make it hard to compare trajectory simplification and/or compression algorithms in a fair circumstance.

Thus, in this paper, we focus on line simplification and let alone map-matching.

[r1] R. Song, W. Sun, B. Zheng, and Y. Zheng. Press: A novel framework of trajectory compression in road networks. PVLDB, 7(9):661--672, 2014.

[r2] R. Gotsman and Y. Kanza. A dilution-matching-encoding compaction of trajectories over road networks.
GeoInformatica, 2015.

[r3] H. Elmeleegy, A. K. Elmagarmid, E. Cecchet and W. G. Aref. Online Piece-wise Linear Approximation of Numerical Streams with Precision Guarantees. PVLDB, 2009.


\textbf{[R1C4]} \emph{ In Page 6 (line 19), it is stated "all points in the simplified trajectory belong to the original trajectory"? Why? If the error bound is the requirement, why we are not allowed to introduce new points, if it helps to improve the compression rate without violating the error bound defined? This is related to my comment C1. }

Yes, we have extended the scope of the paper and allowed insert new data points. Sentences in Page 6 (line 19) have been revised to meet the new scope. Thank you!



\textbf{[R1C5]} \emph{The three distance metrics used in this paper are mainly for line simplification. However, if we apply line simplification to compress trajectories, some of the metrics are no longer that useful. For example, perpendicular distance metric (PED) does not consider temporal dimension, which means the error bounds based on PED consider spatial distances only. However, temporal dimension is very important to trajectories. I think it is necessary to revisit the metrics and highlight their limitations. Particularly, the error-bounds provided by these metrics could be misleading as the metrics do not fully reflect the loss of information. }

Yes, each distance metric has its advantages and limitations. We have revisited them and highlighted their limitations in Section \todo{.}

\textbf{[R1C6]} \emph{ The reason that we decide to store the trajectories (or the compressed version of raw trajectories to save storage space) is mainly to support some applications if required later, which might not require $100\%$ accuracy. For example, in order to find out the witness of a fatal car accident, police might want to locate all the drivers who pass by a specific location within a given time window by querying the trajectories. I am not very sure whether the error bounds based on different distance metrics are still valid for specific types of queries. This survey does not consider the applications to be supported by the compressed trajectories at all. However, personally I think it is very important. The authors did include one set of experiments towards the end of the paper. However, I think it is very important to explain the criteria of trajectory compression in the beginning of the paper, to offer the audience a complete image. In this paper, what are the key considerations when we compress trajectories? Is the compression ratio the only requirement? Is it necessary to support certain applications with error bounds? }

\textbf{[R1C7]} \emph{ The definition of Error Bounded Algorithms is not precise as the error bounds are not presented. }




\textbf{[R1C8]} \emph{ Fig.2 in Page 7 gives an example of reachability graph. However, that graph is incomplete. For example, there is no link between $P_0$ and $P_5$ but they are reachable. This is very misleading. }

\textbf{[R1C9]} \emph{ The datasets used in the experimental study were rather small. With 20M+ points, there is no need to compress them. Much larger datasets will be more realistic for the topic of trajectory compression.  }

\textbf{[R1C10]} \emph{The average errors introduced in Page 20 are derived based on ONLY points contained in a line segment of a piece-wise line representation. Why? Why not consider all the points in the original trajectories? }

\textbf{[R1C11a]} \emph{ In Page 21 (lines 18-19), it is stated that "We then choose 10 trajectories from each dataset, and vary the size |T| of a trajectory from 1,000 points to 10,000 points while fixing the error bound $\epsilon = 40$ metres or $\epsilon = 45$ degrees". It is not clear how points are selected. For example, a raw trajectory contains 100 points (p1, p2, ..., p100). If we want to only consider 10 points, do you consider a sub-trajectory of 10 points (say p1, p2, ..., p10) or a trajectory of much lower sampling rate (say p1, p11, p21, ..., p91). Please state it clearly!}

\textbf{[R1C11b]} \emph{ In Page 21 (lines 44 - 46), it is stated that "The dataset collected by cars (e.g., UCar) also has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cares typically move more regularly than individuals." However, this is different from our everyday observation. Because of the traffic light, cars are not expected to move so regularly. Second, it is different from the observations we could make from Figures 15, 16, 17, 18, 19, and 20. Based on the results reported in those six figures, compression ratios under Geolife and Mopsi are lower than that under UCar, which means the dataset collected by cars has poorer compression ratios. Third, it shall be 'cars' but not 'cares'. }

\textbf{[R1C12]} \emph{ The impact of error bounds on compression ratios is straightforward. The bigger the error bounds, the better the compression ratio. However, the size of trajectory on the compression ratio is not that straightforward. Can the authors please comment on the observations we could make from Figures 18 -20? }

\textbf{[R1C13]} \emph{ In Page 23 (lines 24 - 31), it is stated that "given the same error bound $\epsilon$, the compression ratios of algorithms using PED are obviously better than using SED". This is actually misleading. Although the observation is that algorithms using PED could achieve higher compression ratios, the statement ignores the fact the PED does not consider the temporal dimension at all while SED does. The comparison is not fair at all! If the higher compression ratio is achieved by ignoring the error in the temporal dimension, it has to be stated clearly.} 

\textbf{[R1C14]} \emph{ In Page 27 (lines 44 - 45), it is stated that "When using DAD, the running time from the smallest to the largest is one-pass algorithms Intersect and Interval, batch algorithms TP and DP, and online algorithm OPW." However, algorithms actually perform slightly different at various datasets. It's not accurate to state that online algorithm OPW is always the worst, as it actually performs much better than TP and DP in the dataset Geolife (as shown in Figure 32(2)).}

\textbf{[R1C15]} \emph{ When evaluating the running time of different algorithms under various error bounds, different algorithms demonstrate different trends. The authors might want to explain why algorithms change the trends in certain ways (e.g., why error bounds do not have any impact on the running time of SIPED, why TP, OPW, BQS incur longer running time as error bounds increase, why ......).}

\textbf{[R1C16]} \emph{ Figures 33 - 38 report the average errors of where\_at queries based on trajectories compressed using different line simplification algorithms. The title of 'Evaluation of spatio-temporal queries' is misleading as only one type of queries is considered. In addition, it is necessary to consider when\_at query as it is a critical building block for many applications too.} 

\textbf{[R1C17]} \emph{ Based on the results reported in Figures 33 -38, PED and DAD are actually not proper distance metrics that should be considered when we apply line simplification to compress the trajectories and meanwhile want to preserve the utility of the compressed trajectories. Especially, Figure 21 (avg PED errors under different error bounds) and Figure 33 (avg PED query errors under different error bounds) have different trends (Figure 23 and Figure 35 have different trends too), which further demonstrates that PED and DAD are actually misleading. An algorithm that can achieve good performance in terms of PED/DAD might not be able to guarantee its performance in real applications. }

\textbf{[R1C18]} \emph{ This paper focuses on applying line simplification for trajectory compression. Given the fact that there are different ways to compress trajectories (e.g., [1], [2]), it might be very helpful to give a brief introduction on different trajectory compression techniques, which could help audience to understand the pros/cons of applying line simplification to compress trajectories. }

\emph{ [1] CiNCT: Compression and Retrieval for Massive Vehicular Trajectories via Relative Movement Labeling. Satoshi Koide, Yukihiro Tadokoro, Chuan Xiao, Yoshiharu Ishikawa. ICDE'2018.}

\emph{ [2] COMPRESS: A Comprehensive Framework of Trajectory Compression in Road Networks. Yunheng Han, Weiwei Sun, Baihua Zheng. TODS 2017.}


%******************* reviewer 2 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 2.}

\line(1,0){100}

\textbf{[R2D1]} \emph{In page 2, the authors mention that “important aspects of trajectory simplification (compression ratios, running time and aging friendliness) are not systematically studied” for [41]. Not sure which “important aspects” the authors refer to, as I found running time and the tradeoff between compression ratios and errors have been reported in [41].}

\textbf{[R2D2]} \emph{In page 3: $epsilon_1$ and $epsilon_2$ appeared without explanation.}

\textbf{[R2D3]} \emph{ In page 4: It’s better to move the comparison with [41] to the motivation part, in order to assure the necessity of a new empirical study.}

\textbf{[R2D4]} \emph{The authors mention that one of the main contributions is to re-implement the methods with Java. So compared with the running time results reported in [41], any new or different findings are revealed?}

\textbf{[R2D5]} \emph{In Table 1, what are the criteria for the selection of “representative” algorithms? The authors may need to explicitly explain this in this paper.}

\textbf{[R2D6]} \emph{Obviously, data aging is a very important contribution in this paper, because it was not examined in [41]. However, the situation becomes very awkward when most of the existing algorithms are not data aging friendly. Among all the methods in Table 1, only DP (with SED and PED as the distance measure) is data aging friendly. If a user considers data aging is important, then he/she has no choice but to use DP. The authors may need to provide more convincing arguments for the necessity of examining data aging.}


\textbf{[R2D7]} \emph{In this paper, only three types of distance measure PED, SED and DAD are presented and compared. The other distance measures are briefly touched in the Appendix. Question here is that method like Dots is recommended for online compression in [41] as it shows better trade-off in terms of compression ratio and error. So when recommending users with suitable compression algorithms, would the guidance be biased if the authors exclude certain existing methods?}

\textbf{[R2D8]} \emph{In page 23: the authors mentioned that “in practice, SED has obviously better compression ratios than DAD..”. This is confusing to me because the distance unit for SED is meter and unit for DAD is degree. How can they be compared? It seems like the effect of epsilon=100m in SED is similar to epsilon=60 degree in DAD?}

\textbf{[R2D9]} \emph{Besides clarifying the differences with [41] in the introduction, it’s better to summarize the differences of experimental findings at the end of the paper, e.g., from the experimental results, what are the new insights not covered in [41] and what are the different/conflicting findings?}

%******************* reviewer 3 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 3.}

\line(1,0){100}

\textbf{[R3C1]} \emph{
Sec. 4.1. The min-\# optimal version from [4] has different complexities depending on whether the polyline is open or closed (as well as convex vs. concave). The authors have not properly brought this in context.
Also, in multiple places, the authors are bringing the notion of the "3D" - however, they do not mention the reference [r1] which was the first one to extend the algorithm in [4] to 3D. }

\emph{[r1] Gill Barequet, Danny Z. Chen, Ovidiu Daescu, Michael T. Goodrich, and Jack Snoeyink. Efficiently approximating polygonal paths in three and higher dimensions. Algorithmica, 33(2):150–167, 2002.}

\textbf{[R3C2]} \emph{
Sec. 4.2. One aspect that is context-dependent (and often implicitly ignored) in online algorithms, is the communication overhead (which, in turn, may imply energy consumption in sensor networks); along with the complementary aspect of "freshness" of the compressed data in the respective sink. Please see [r2] below.}

\emph{[r2] Oliviu Ghica, Goce Trajcevski, Ouri Wolfson, Ugo Buy, Peter Scheuermann, Fan Zhou, Dennis Vaccaro: Trajectory Data Reduction in Wireless Sensor Networks. IJNGC 1(1) (2010)
}

\textbf{[R3C3]} \emph{
Sec. 4.3. Actually [13] does provide a kind of a taxonomy, and it would be nice to compare it with the present article. Also, [37] has a claim/proof that if the online version uses $\varepsilon$ as an error-bound in the LDR, then the final outcome is an equivalent to a compression of $2 \cdot \varepsilon$ applied to the entire historic trajectory (if it were available). This, in a sense, is a form of a justification  for the use of $\varepsilon$/2 in works like CISED ([14]).}

\textbf{[R3C4]} \emph{
In several places, the authors are making statements that are not properly phrased, and some that can be taken out of context. Examples:}

\emph{1. The authors should try to define terms before their first use. Example: on p.1 in the Introduction, the "min-\#" is brought up, without properly explaining its meaning.}


\emph{2. p.2: "...no need of extra knowledge and suitable for freely moving objects [28]...". A statement of this sort does not do justice to [28].
Namely, [28] considers what happens with compression if the trajectories are constrained to move along road networks - but, it also is based on the motivation that in such settings there may be a different perspective - i.e., the overall-compression of the DB.}



\emph{3. p.33: the authors make a claim to the effect of: "These methods lack the capability of compressing..." when describing the min-$\epsilon$ variant. The statement needs to be clarified, or put in the more appropriate contexts: the min-$\epsilon$  is a complementary one to min-\# and, as such, it never makes any other claim that it will yield the minimum error, given a fixed "budget" of m points. However, that does not mean that this variant of the problem, and the algorithmic solution, doesn't have its own merits.}



\line(1,0){500}



Your sincerely,

Xuelian Lin, Shuai Ma, Yanchen Hou, Yihao Fu and Tianyu Wo

%\bibliographystyle{abbrv}
%\bibliography{sec-ref}


\end{document}
