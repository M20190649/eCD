\documentclass{letter}
\usepackage{geometry}

% duan
\usepackage{xspace}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{multirow,amsmath, array,colortbl}


\newcommand{\marked}[1]{\textcolor{red}{#1}}

\newcommand{\kw}[1]{{\ensuremath {\mathsf{#1}}}\xspace}

\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}
\newcommand{\kwlog}{\emph{w.l.o.g.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\sstab}{\rule{0pt}{8pt}\\[-2.4ex]}

\newcommand{\topk}[1]{\kw{top}--\kw{#1}}
\newcommand{\topdown}{\kw{topDown}}
\newcommand{\extsubgraph}{\kw{compADS^+}}
\newcommand{\drfds}{\kw{FIDES^+}}
\newcommand{\extsubgraphold}{\kw{compADS}}
\newcommand{\findtimax}{\kw{maxTInterval}}
\newcommand{\findtimin}{\kw{minTInterval}}
\newcommand{\meden}{\kw{MEDEN}}

\newcommand{\tranformgraph}{\kw{convertAG}}
\newcommand{\mergecc}{\kw{strongMerging}}
\newcommand{\strongpruning}{\kw{strongPruning}}
\newcommand{\boundedprobing}{\kw{boundedProbing}}

\newcommand{\AFPR}{\kw{AFP}-\kw{reduction}}
\newcommand{\nwm}{{\sc nwm}\xspace}


\newcommand{\cone}[1]{{$\mathcal{C}{#1}$}}
\renewcommand{\circle}[1]{{$\mathcal{O}{#1}$}}
\newcommand{\pcircle}[1]{{$\mathcal{O}^c{#1}$}}

\newcommand{\vv}{\overrightarrow}


\newcommand{\todo}[1]{\textcolor{red}{Todo...#1}}
\begin{document}





Prof. {Chris Jermaine} \\
Editor-in-Chief		\\
ACM TODS	\\



Dear Prof. Jermaine,

Attached please find a revised version of our submission to
the TODS, \emph{Error Bounded Line Simplification Algorithms for Trajectory Compression: An Experimental Evaluation}.


%%%{\textbf{[Comments from Editors]}.\emph{These reviews, by recognized experts in the field, have obviously been prepared with care. Based on the reviews, the recommendation by handling Associate Editor Dr. Seeger, and my own assessment, I find that the paper needs to undergo a successful major revision to be acceptable for publication in TODS. As Dr. Seeger wrote to me, the reviews are quite consistent: two of them ask for a major revision, one for a minor revision. All of them agree that the survey paper is well written. Reviewer 1 rises most concerns regarding the variety of methods considered (the paper does not consider techniques where new points are used for compression), not considering map matching as a preprocessing step, error metrics, impact on applications, and the small data sets used in the experiments. The authors should carefully consider these points.}}


The paper has been substantially revised according to the comments of you, Dr. Seeger and referees. In particular, we have added{
	(a) techniques that allow data interpolation (namely, weak simplification), \eg~\kw{CISED}-{W},
	(b) techniques that use the Local Integral Square SED (LISSED) as error measure,  \eg~\kw{DOTS},
	(c) experimental studies on an additional application named ``when\_at" query, 
	(d) a larger dataset of UCar in the experimental studies, and
	(e) a brief introduction to map-matching-based trajectory compression methods in appendix.}
Besides, an explanation on ``why map-matching is not considered as a pre-processing step of trajectory simplification" is provided in {[R1C3]}.


We would like to thank you, Dr. Seeger and all the referees for your earnestness and conscientiousness, and for your valuable comments.

Below please find our detailed responses to the comments.



%******************* reviewer 1 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 1.}

\line(1,0){100}


\textbf{[R1C1]} \emph{What is ``min-\# problem" mentioned in Section 1 (second paragraph)?}


The \emph{$min-\#$ problem} of trajectory simplification is, given a curve (\ie an original trajectory) and an $\epsilon$, to construct an approximate curve with error within $\epsilon$ and, at the same time, having the minimum number of line segments.

We have explained it in Section 1 (the second paragraph). 
Thanks for pointing out this!

\textbf{[R1C2]} \emph{It is stated in the paper that "The idea of piece-wise line simplification (LS) comes from computational geometry, whose target is to approximate a fine piece-wise linear curve with a coarse one (whose corresponding data points are a subset of the original one), such that the maximum distance of the former to the latter is bounded by a user specified threshold." This is actually misleading. Line simplification actually could be performed by using points other than original ones to represent the original line. The authors did mention those algorithms which consider points outside the original trajectories in the appendix. However, to make sure the correctness of the statements, it's better to mention that in the beginning of the paper. In addition, it is necessary to explain why this paper only considers the line simplification algorithms that consider the original points only. }

Yes, line simplification could use points other than original ones to represent the original line. Indeed, they are \emph{strong simplification} that outputs data points only belonging to the input trajectory, and \emph{weak simplification} that allows insert new data points into the output trajectories.

(1) We have correct the statements in the beginning of the paper (the second paragraph of Section 1), where ``(whose corresponding data points are a subset of the original one)" is replaced by ``(if all data points of the coarse curve are a subset of the original curve, then it's a strong simplification; otherwise, it's a weak simplification)".

(2) Now, this paper has covered both strong and weak simplifications, more specifically, we have tested and analyzed ``weak simplification" algorithms (OPERB-A and CISED-W) in Section 6. 

Thanks for pointing out this!

\textbf{[R1C3]} \emph{For trajectories that capture the moving objects' movement along the road network, the points are normally mapped to road networks. In other words, it is not necessary to use piece-wise line representation. It looks like the authors have not considered this case, which is actually one of the most common cases. Can the authors please explain why map-matching step is not considered even for trajectories that capture the movements in road networks?}


We strongly believe that \emph{map-matching is a task parallel to line simplification}, and should not be the pre-processing step of the later. The reasons are as follows:

(1) Trajectory simplification is better performed as early as possible, \ie it's better to run trajectory simplification on mobile devices that collect GPS points, such that not only storage of data servers but also network bandwidth between mobile devices and data servers is saved. 
In this case, line simplification is more suitable to run on those resource-constrained end devices than road-aware trajectory compression, as it is  simple and easy to implement, and does not require extra information like road network. Thus, map-matching is not necessary the pre-step of trajectory simplification.

(2) Line representation is a simple and natural way to represent a trajectory, and we can freely match an original or simplified trajectory onto road network at anytime. This is important as moving objects often move from city to wild, or move in areas outside urban where no fine or up-to-date road network is available. In these cases, if we let map-matching be the pre-step of trajectory simplification, it may introduce errors caused by mis-map-matching. However, if we save the line representation of these trajectories (simplified), then once the road network is available or updated (this can happen many times), they can also be matched onto roads when we need. Here, map-matching is better not the pre-processing step of trajectory simplification too. 

(3) We have also observed that there is a kind of trajectory compression called road-aware trajectory compression [r1, r2] that is used to compress trajectories that capture the moving objects' movement along the road network. Indeed, it has at least two technical routines, the first one maps a trajectory onto roads then simplifies those mapped data points by \emph{numerical streams} method [r3], such as PRESS [r1] does; and the second one first simplifies the trajectory by some line simplification method, then maps the result onto roads, such as the dilution-matching-encoding method [r2] does. In the first case, map-matching is the pre-step of compression, while \emph{in the second case, line simplification is the pre-step of map-matching.} 
%

%By this way, the mis-map-matching is alleviated.

Thus, in this evaluation, we focus on line simplification and let alone map-matching. 
%To clearify this, we have added a brief introduction to map-matching-based trajectory compression techniques in ``Appendix: Additional Trajectory Compression Algorithms". 


[r1] R. Song, W. Sun, B. Zheng, and Y. Zheng. Press: A novel framework of trajectory compression in road networks. PVLDB, 7(9):661--672, 2014.

[r2] R. Gotsman and Y. Kanza. A dilution-matching-encoding compaction of trajectories over road networks.
GeoInformatica, 2015.

[r3] H. Elmeleegy, A. K. Elmagarmid, E. Cecchet and W. G. Aref. Online Piece-wise Linear Approximation of Numerical Streams with Precision Guarantees. PVLDB, 2009.


\textbf{[R1C4]} \emph{ In Page 6 (line 19), it is stated ``all points in the simplified trajectory belong to the original trajectory"? Why? If the error bound is the requirement, why we are not allowed to introduce new points, if it helps to improve the compression rate without violating the error bound defined? This is related to my comment C1. }

Yes, we have allowed interpolate new points into the output trajectories, and we have replaced the sentence ``That is to say, all points in the simplified trajectory belong to the original trajectory" by ``That is to say, {data interpolation is allowed." You can find it in the last paragraph of Section 2, also refer to [R1C2] for more details. Thank you!


\textbf{[R1C5]} \emph{The three distance metrics used in this paper are mainly for line simplification. However, if we apply line simplification to compress trajectories, some of the metrics are no longer that useful. For example, perpendicular distance metric (PED) does not consider temporal dimension, which means the error bounds based on PED consider spatial distances only. However, temporal dimension is very important to trajectories. I think it is necessary to revisit the metrics and highlight their limitations. Particularly, the error-bounds provided by these metrics could be misleading as the metrics do not fully reflect the loss of information. }

Yes, when researchers introduce PED to simplify trajectories, they basically treat the input trajectory as a sequence of spatial data points. As the result, \emph{line simplification algorithms using PED bring good compression ratios at a cost of losing temporal information of trajectories}. Hence, though it is error bounded by PED, it is not spatio-temporal \emph{query friendly}, \ie a spatio-temporal query like \emph{where\_at} on such a simplified trajectory is possible return a point that has a (SED) distance great larger than the PED error bound used in the simplification algorithm. DAD is the direction deviation of the moving object, and 
temporal information is also lost when using DAD, therefor, it is \emph{not spatio-temporal query friendly} too.

To clarify this, we have revised the descriptions of distance metrics PED, SED and DAD, and also highlighted their limitations, in the part of ``distance metrics", in Section 1. Thanks!

\textbf{[R1C6]} \emph{ The reason that we decide to store the trajectories (or the compressed version of raw trajectories to save storage space) is mainly to support some applications if required later, which might not require $100\%$ accuracy. For example, in order to find out the witness of a fatal car accident, police might want to locate all the drivers who pass by a specific location within a given time window by querying the trajectories. I am not very sure whether the error bounds based on different distance metrics are still valid for specific types of queries. {This survey does not consider the applications to be supported by the compressed trajectories at all. However, personally I think it is very important.} The authors did include one set of experiments towards the end of the paper. However, I think it is very important to explain the criteria of trajectory compression in the beginning of the paper, to offer the audience a complete image. In this paper, what are the key considerations when we compress trajectories? Is the compression ratio the only requirement? Is it necessary to support certain applications with error bounds? }

(1) The above example is indeed a case of spatio-tempporal \emph{intersect} query (also called a spatio-temporal \emph{range} query) [r4], defined in $intersect$ ($T$, \emph{Poly}, $t_1$, $t_2$), that returns \emph{true} if the trajectory $T$ intersects the polygon \emph{Poly} between the times $t_1$ and $t_2$. Authors in [r4] have proved that the SED-simplified trajectory is answer-error-bound for spatio-temporal queries (\emph{query friendly} in short) of \emph{where\_at, intersect} and \emph{nearest\_neighbor}, \ie if the simplification is SED error bounded by $\epsilon$, then the answer to such query is also error bounded by $\epsilon$.
However, if PED is used, then the answer to a spatio-temporal query on simplified trajectories is not error bounded [r4], \ie PED is not query friendly. Though DAD is not discussed in [r4] (DAD is developed after the publish of [r4]), it's obviously not spatio-temporal query friendly too. Our tests in Section 6.3.5 also support these statements.

(2) Yes, it's important to consider applications on the top of simplified trajectories. Thus, we have evaluated line simplification algorithms and distance metrics with respect to spatio-temporal queries \emph{where\_at} and \emph{when\_at}, two fundamental blocks of spatio-temporal queries. Note that [r4] have proved that ``the errors of the \emph{intersect} and \emph{nearest\_neighbor} query types are bounded if and only if the error of \emph{where\_at} is bounded", which indicates that the \emph{where\_at} query is one of the keys to test spatio-temporal queries; and \emph{when\_at} is another important functional block that is often discussed in comparing with \emph{where\_at}.
We have clarified this in Section 6.3.5.

(3) {Yes, it is important to explain the criteria of trajectory compression to offer the audience a complete image.} Thus, we have added a part of content named ``Quality criteria" in Section 1. Indeed, when we talk about the quality criteria of trajectory simplification, there are two aspects, 
(a) the first one comes from trajectory simplification itself, including compression ratio, efficiency and errors of simplification. The works in this area all test their algorithm following a part or all of these criteria, and 
(b) the second one is from trajectory applications, \eg spatio-temporal queries, map-matching, trajectory clustering, anonymous and so on. Indeed, every trajectory application is applicable to evaluate the simplified trajectories from its point of view, such as spatio-temporal queries concern the answer-errors and a map-matching or a trajectory clustering method concerns the accuracy.
%
In this paper, we follow the mainstream works in this area and test the compression ratios, efficiency and errors of simplification algorithms. For trajectory application, we choose spatio-temporal queries as the representatives as they are well-known and commonly used in the management of trajectories. Besides, as mentioned in Section 5, the data aging problem is a potential challenge to the management of trajectory, thus, the average and max errors of trajectory simplification algorithms as well as distance metrics in data aging are also tested.
%
We believe these are also the key considerations when we compress trajectories. Besides, compression ratio is not the only requirement as mentioned above, and it is sure necessary to support certain applications with error bounds, \eg spatio-temporal queries of \emph{where\_at}, \emph{intersect} and \emph{nearest\_neighbor}.

Thanks for your great advice!

[r4] Cao, H., Wolfson, O., and Trajcevski, G. Spatio-temporal data reduction with deterministic error bounds. VLDBJ 15, 3 (2006), 211–228.

\textbf{[R1C7]} \emph{ The definition of Error Bounded Algorithms is not precise as the error bounds are not presented. }

We have revised the definition of error bounded algorithms in the penultimate paragraph of Section 2. That is, given a trajectory $\dddot{\mathcal{T}}\left[P_0, \dots, P_n\right]$ and a pre-specified bound $\epsilon$,
trajectory simplification algorithm $\mathcal{A}$ using PED (respectively, SED and DAD) is \emph{error bounded} by $\epsilon$ if for each point $P_k$ ($k\in[0,n]$) in $\dddot{\mathcal{T}}$, there exists a line segment $\mathcal{L}_i = \vv{P'_{s_i}P'_{e_i}}$ in $\overline{\mathcal{T}}$ with $s_i \le k \le e_i$ ($0\le i\le m$) such that the PED distance $ped\left(P_k, \mathcal{L}_i\right)$  (respectively the SED distance $sed\left(P_k, \mathcal{L}_i\right)$ and the DAD distance $dad\left(\vv{P_{k}P_{k+1}}, \mathcal{L}_i\right)$) is no more than  $\epsilon$.

Thanks for pointing out this!


\textbf{[R1C8]} \emph{ Fig.2 in Page 7 gives an example of reachability graph. However, that graph is incomplete. For example, there is no link between $P_0$ and $P_5$ but they are reachable. This is very misleading. }

Yes, to avoid the misleading, we have moved up point $P_4$ such that the PED distances from $P_4$ to line segments $\overline{P_0P_5}$, $\overline{P_1P_5}$, $\overline{P_2P_5}$ and $\overline{P_3P_5}$ are all larger than the error bound. Besides, all running examples have been revised.
Thanks for your seriousness and meticulousness!

%\begin{center}
%	%\includegraphics[width=1.\textwidth]{example-image}
%	\includegraphics[scale=0.75]{reachgraph.png}
%	\label{fig:rgraph}
%\end{center}

%As shown below, if we connect $P_0$ and $P_5$, then the \kw{PED} from $P_4$ to line segment $\overline{P_0P_5}$, \ie $|P_4P'_4|$, is a bit larger than the error bound $\epsilon$ (exact two frames in the figure). Thus, $P_5$ is not reachable from $P_0$, and there should be no link between them. 




\textbf{[R1C9]} \emph{ The datasets used in the experimental study were rather small. With 20M+ points, there is no need to compress them. Much larger datasets will be more realistic for the topic of trajectory compression.  }

Yes, we have extended dataset UCar from 200 to 1000 trajectories such that it totally has more than {100M} points. Besides, (1) we have already used the full datasets of Geolife and Mopsi, which are widely used in many recent trajectory compression works, \eg Geolife is used in the recent algorithm algorithm CISED [r5] and experimental study [r6], and Mopsi is used in [r5], and (2) some long trajectories in these datasets each has more than {one million} of data points which is quite memory consuming as the whole trajectory must be loaded in memory before the bath algorithms start to run. Also, due to the relatively high time complexities of optimal, batch and online algorithms, \eg $O(n^2)$ of both Douglas-Peucker and Theo-Pavlidis, we already need {days} to run the whole tests listed in Section 6. %Thus, an even larger datasets is quite not friendly to the tests.
Thanks for your advice!

[r5] Lin, X., Jiang, J., Ma, S., Zuo, Y., and Hu, C. One-pass trajectory simplification using the synchronous euclidean distance. VLDBJ 28, 6 (2019), 897–921.

[r6] Zhang, D., Ding, M., Yang, D., Liu, Y., Fan, J., and Shen, H. T. Trajectory simplification: An experimental study and quality analysis. PVLDB 9, 11 (2018), 934–946.

\textbf{[R1C10]} \emph{The average errors introduced in Page 20 are derived based on ONLY points contained in a line segment of a piece-wise line representation. Why? Why not consider all the points in the original trajectories? }

Indeed, we do consider ALL the points in the original trajectories when calculate the average errors. To clarify this, we have revised the description of ``average errors" in Section 6.2 to a more clear statement ``{The average simplification error is the average value of the distances from every point of the original trajectories to its representing line segment of the simplified trajectories.}". Thanks for points out this!


\textbf{[R1C11a]} \emph{ In Page 21 (lines 18-19), it is stated that "We then choose 10 trajectories from each dataset, and vary the size |T| of a trajectory from 1,000 points to 10,000 points while fixing the error bound $\epsilon = 40$ metres or $\epsilon = 45$ degrees". It is not clear how points are selected. For example, a raw trajectory contains 100 points (p1, p2, ..., p100). If we want to only consider 10 points, do you consider a sub-trajectory of 10 points (say p1, p2, ..., p10) or a trajectory of much lower sampling rate (say p1, p11, p21, ..., p91). Please state it clearly!}

It is the Top-K points of a trajectory (\eg, the Top-100 points are $P_1, P_2, P_3, ..., P_{100}$). We have clarified this in the last paragraph of Section 6.1.

\textbf{[R1C11b]} \emph{ In Page 21 (lines 44 - 46), it is stated that ``The dataset collected by cars (e.g., UCar) also has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cares typically move more regularly than individuals." However, this is different from our everyday observation. Because of the traffic light, cars are not expected to move so regularly. Second, it is different from the observations we could make from Figures 15, 16, 17, 18, 19, and 20. Based on the results reported in those six figures, compression ratios under Geolife and Mopsi are lower than that under UCar, which means the dataset collected by cars has poorer compression ratios. Third, it shall be 'cars' but not 'cares'. }


Yes, cars are not expected to move so regularly (in speed). And partially because of this and the relative lower average sampling rate of dataset UCar, it has poorer compression ratios than Geolife and Mopsi when use Euclidean distance metrics PED and SED, as shown in Figures 16, 17, 19 and 20 (corresponding to Figures 15,16, 18 and 19 in the preview manuscript). However, when use the direction metric DAD, dataset UCar do have better compression ratios than Geolife and Mopsi, as shown in Figures 18 and 21 (corresponding to Figures 17 and 20 in the preview manuscript), because cars usually move more regularly than individuals {in directions}.

To clarify this, in the second paragraph of Section 6.3.1, we have revised the sentence ``The dataset collected by cars (e.g., UCar) also has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cares typically move more regularly than individuals" to ``\textcolor{blue}{When use DAD,} the dataset collected by cars (e.g., UCar) has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cars typically move more regularly than individuals \textcolor{blue}{in directions}."
Thanks for pointing out these!


\textbf{[R1C12]} \emph{The impact of error bounds on compression ratios is straightforward. The bigger the error bounds, the better the compression ratio. However, the size of trajectory on the compression ratio is not that straightforward. Can the authors please comment on the observations we could make from Figures 18 -20? }

It seems that data size does not have noteworthy impacts on compression ratios. As we know, a long trajectory is connected by a number of sub-trajectories $[\dddot{T}_1,\dddot{T}_2,..., \dddot{T}_m, ]$ that order by time, and each sub-trajectory $\dddot{T}_i, i \in [1,m]$, can be simplified to several line segments. Then suppose this original trajectory is being simplified by some online or one-pass algorithm that runs in an incremental manner, it is reasonable to conclude that the appending of $\dddot{T}_{m+1}$ may have a bit impacts of the representation (simplification) of $\dddot{T}_{m}$, but it's hard to have impacts on $\dddot{T}_i$ before $\dddot{T}_{m}$. As a result, data size does not have noteworthy impacts on compression ratios. For batch and the optimal algorithms, our tests reveal a similar phenomenon.

We have state this observation in the third paragraph of Section 6.3.1 (the 2{nd} item). Thanks!



\textbf{[R1C13]} \emph{ In Page 23 (lines 24 - 31), it is stated that "given the same error bound $\epsilon$, the compression ratios of algorithms using PED are obviously better than using SED". This is actually misleading. Although the observation is that algorithms using PED could achieve higher compression ratios, the statement ignores the fact the PED does not consider the temporal dimension at all while SED does. The comparison is not fair at all! If the higher compression ratio is achieved by ignoring the error in the temporal dimension, it has to be stated clearly.} 

Yes, PED has better compression ratios than SED at a price of losing temporal information. We have clarified this in the penultimate paragraph of Section 6.3.1, say, ``\textcolor{blue}{because SED saves temporal information while PED does not (remember the losing of temporal information may lead to unexpected results, \eg unbounded answer-errors to spatio-temporal queries)}, given the same error bound $\epsilon$, the compression ratios of algorithms using PED are obviously better than using SED." 
Besides, the limitations of each distance metric are also discussed in the ``distance metrics" part of Section 1. Thanks!

\textbf{[R1C14]} \emph{ In Page 27 (lines 44 - 45), it is stated that "When using DAD, the running time from the smallest to the largest is one-pass algorithms Intersect and Interval, batch algorithms TP and DP, and online algorithm OPW." However, algorithms actually perform slightly different at various datasets. It's not accurate to state that online algorithm OPW is always the worst, as it actually performs much better than TP and DP in the dataset Geolife (as shown in Figure 32(2)).}

We have corrected the statement. This sentence is replaced by ``When using DAD, one-pass algorithms Intersect and Interval run prominently faster than batch algorithms TP and DP and online algorithm OPW." Thanks!

\textbf{[R1C15]} \emph{ When evaluating the running time of different algorithms under various error bounds, different algorithms demonstrate different trends. The authors might want to explain why algorithms change the trends in certain ways (e.g., why error bounds do not have any impact on the running time of SIPED, why TP, OPW, BQS incur longer running time as error bounds increase, why ......).}

Yes, that's it! We believe it is helpful for users to understand the characters of the algorithms, and to choose an appropriate algorithm that meets the needs.

\textbf{[R1C16a]} \emph{ Figures 33 - 38 report the average errors of where\_at queries based on trajectories compressed using different line simplification algorithms. The title of 'Evaluation of spatio-temporal queries' is misleading as only one type of queries is considered.} 

Yes, we have revised the titles. Thanks for pointing out this!

\textbf{[R1C16b]} \emph{In addition, it is necessary to consider \emph{when\_at} query as it is a critical building block for many applications too.} 

Yes, we have added new tests \wrt~the \emph{``when\_at"} query in Section 6.3.5. Thanks for your advice!

\textbf{[R1C17]} \emph{ Based on the results reported in Figures 33 -38, PED and DAD are actually not proper distance metrics that should be considered when we apply line simplification to compress the trajectories and meanwhile want to preserve the utility of the compressed trajectories. Especially, Figure 21 (avg PED errors under different error bounds) and Figure 33 (avg PED query errors under different error bounds) have different trends (Figure 23 and Figure 35 have different trends too), which further demonstrates that PED and DAD are actually misleading. An algorithm that can achieve good performance in terms of PED/DAD might not be able to guarantee its performance in real applications. }

Yes, an algorithm using PED/DAD is not able to guarantee the error bounds of spatio-temporal queries, which makes it inappropriate in scenarios that spatio-temporal queries are required. We have highlighted this in the penultimate paragraph of Section 6.3.6 (also refer to the discusses in ``Quality criteria and distance metrics" of Section 1). Thanks!

\textbf{[R1C18]} \emph{ This paper focuses on applying line simplification for trajectory compression. Given the fact that there are different ways to compress trajectories (e.g., [1], [2]), it might be very helpful to give a brief introduction on different trajectory compression techniques, which could help audience to understand the pros/cons of applying line simplification to compress trajectories. }

\emph{ [1] CiNCT: Compression and Retrieval for Massive Vehicular Trajectories via Relative Movement Labeling. Satoshi Koide, Yukihiro Tadokoro, Chuan Xiao, Yoshiharu Ishikawa. ICDE'2018.}

\emph{ [2] COMPRESS: A Comprehensive Framework of Trajectory Compression in Road Networks. Yunheng Han, Weiwei Sun, Baihua Zheng. TODS 2017.}


{Yes, we have added a brief introduction to the semantic-based methods in ``Appendix: Additional Trajectory Compression Algorithms". Thanks!} 
%The relationship between them and line simplification is also discussed.

%******************* reviewer 2 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 2.}

\line(1,0){100}

\textbf{[R2D1]} \emph{In page 2, the authors mention that “important aspects of trajectory simplification (compression ratios, running time and aging friendliness) are not systematically studied” for [41]. Not sure which “important aspects” the authors refer to, as I found running time and the trade-off between compression ratios and errors have been reported in [41].}

We have revised the motivation part of the manuscript to clarify this.
Roughly speaking, \emph{error bounds} and \emph{trajectories} are the basic two inputs of any trajectory simplification algorithm solving the ``min-\#" problem, thus, a comprehensive evaluation should test the impacts of them on the effectiveness and efficiency of those algorithms.  
%
However, we have noticed that [41] ([58] in the revised version) (1) only has a limited evaluation on running time. It does show a table that lists \emph{compression time per trajectory point} of each algorithm, but it is not sufficient to guide users, as the impacts of both the sizes of trajectories and the error bounds is ignored by [41], and (2) only has a limited evaluation on compression ratios. It does report the relationship between compression ratios and errors (note, they are errors, not error bounds), but the more important aspect in practice, the impacts of the error bounds on the compression ratios, is ignored. Thus, it is also hard to guide users to set reasonable error bounds.
%
Besides, some important algorithms and the data aging problem are not covered in [41]. These are the reasons that we say ``important aspects of trajectory simplification (compression ratios, running time and aging friendliness) are not \emph{systematically} studied” for [41]. 
Thanks for pointing out this!

\textbf{[R2D2]} \emph{In page 3: $epsilon_1$ and $epsilon_2$ appeared without explanation.}

We have fixed it. Thanks for pointing out this!

\textbf{[R2D3]} \emph{ In page 4: It’s better to move the comparison with [41] to the motivation part, in order to assure the necessity of a new empirical study.}

Yes, we have moved some comparisons with [41] to the motivation part (also see [R2D1]). Thanks for your advice!
%that is helpful to elaborate the motivation of the work

\textbf{[R2D4]} \emph{The authors mention that one of the main contributions is to re-implement the methods with Java. So compared with the running time results reported in [41], any new or different findings are revealed?}

First of all, we re-implement the methods with the same language, Java, for a fair comparison.
As a result, we find that algorithm DOTS runs slowly in Java when the input trajectory is large.
Please refer to Section 6.3.6 for more details. Thanks


\textbf{[R2D5]} \emph{In Table 1, what are the criteria for the selection of “representative” algorithms? The authors may need to explicitly explain this in this paper.}

Yes, we have added a brief explanation of the criteria after Table 1. Besides, we have also explained the basis of selections in the top paragraphs of Sections 3, 4.1, 4.2 and 4.3.

\textbf{[R2D6]} \emph{Obviously, data aging is a very important contribution in this paper, because it was not examined in [41]. However, the situation becomes very awkward when most of the existing algorithms are not data aging friendly. Among all the methods in Table 1, only DP (with SED and PED as the distance measure) is data aging friendly. (1) {If a user considers data aging is important, then he/she has no choice but to use DP}. (2) {The authors may need to provide more convincing arguments for the necessity of examining data aging}.}

We have clarified these in Section 5, and we would like to summarize the answers to the comments here.

(1) \underline{``If a user considers data aging is important, then he/she has no choice but to use DP"}: \emph{Data aging} is important while \emph{aging friendly} is not so important, say, \emph{data aging} is rather an important requirement derived from the management of trajectory data, and it is not identical to \emph{aging friendly}. Thus, when we talk about data aging, it is not necessary to choose the \emph{aging friendly} algorithm (DP) to compress trajectories --- though given the same error bounds, it may bring better compression ratios in data aging. Also notice that compression ratios is only one consideration on choosing an algorithm. There are also errors and efficiency that impact the selection of algorithms.
%That is to say, any algorithm could be used in data aging. 

(2) \underline{``The authors may need to provide more convincing arguments for the necessity of examining data aging"}: Majorly, we have proved two important things \wrt data aging, (a) most algorithms are not \emph{aging friendly} (Section 5.1), and (b) all algorithms are \emph{aging safe} (Section 5.2), \ie let $\mathcal{A}$ be a line simplification algorithm,  $\mathcal{M}$ be a distance metric, and $\epsilon_1>0$ and $\epsilon_2>0$ be error bounds, algorithm $\mathcal{A}$ is \emph{aging safe} if the errors between original trajectory $\dddot{\mathcal{T}}$ and simplified trajectory $\overline{\mathcal{T}}=\mathcal{A}(\mathcal{A}(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{M}), \epsilon_2, \mathcal{M})$ are not more than $\epsilon_1+ \epsilon_2$. Which means that, in data aging, \emph{one can freely re-compress trajectories by any algorithm as long as he uses the same distance metric.} The aging friendliness and aging safety of algorithms (Table 3) are two important findings of the work, these together provide a full picture of the data aging problem, and let user know how to choose an algorithm and set the parameter of error bound in data aging. 
%, and also serves as the reason of examining data aging


\textbf{[R2D7]} \emph{In this paper, only three types of distance measure PED, SED and DAD are presented and compared. The other distance measures are briefly touched in the Appendix. Question here is that method like Dots is recommended for online compression in [41] as it shows better trade-off in terms of compression ratio and error. So when recommending users with suitable compression algorithms, would the guidance be biased if the authors exclude certain existing methods?}

We have included the local integral square SED (LISSED) and evaluated algorithms OptLISSED, MRPA, DOTS and OLTS using LISSED in the revised version. In particular, we have (1) briefly introduced LISSED in Section 2, after the definition of the Synchronous Euclidean Distance (SED), (2) selected DOTS as the representative of these algorithms and described it in Section 4.2.4, and (3) reported our findings about DOTS and LISSED in Section 6.
Also note that, in this revision, {the LISSED is rather a special SED-based ``error measure" or a methodology that efficiently calculate SED errors than a kind of ``distance measure"}.

\textbf{[R2D8]} \emph{In page 23: the authors mentioned that “in practice, SED has obviously better compression ratios than DAD..”. This is confusing to me because the distance unit for SED is meter and unit for DAD is degree. How can they be compared? It seems like the effect of epsilon=100m in SED is similar to epsilon=60 degree in DAD?}

First of all, it is helpful to give readers an intuitive impression about the performance of DAD compared with SED. However, as you have mentioned, SED is a Euclidean distance metric, having a value in $[0, \infty]$, and DAD is a direction metric, having a value in $[0, 360]$ degrees, hence, it is hard to compare them under absolutely fair conditions. 
Alternatively, we suppose there are two practical scenarios, one uses SED with $\epsilon  \le  100$ meters, and the other uses DAD with $\epsilon \le 60$ degrees, then we compare the performance of them, \eg the performance of SED with $\epsilon=100$ meters vs. that of DAD with $\epsilon=60$ degrees. We find that in these comparisons, SED usually have obviously better compression ratios than DAD.

{We have revised the last paragraph of Section 6.3.1 to clarify this. Thanks!}

\textbf{[R2D9]} \emph{Besides clarifying the differences with [41] in the introduction, it’s better to summarize the differences of experimental findings at the end of the paper, e.g., from the experimental results, what are the new insights not covered in [41] and what are the different/conflicting findings?}

We have summarized the new or conflicting findings compared with [41] at the end of Section 6.3.6.


%******************* reviewer 3 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 3.}

\line(1,0){100}

\textbf{[R3C1a]} \emph{
Sec. 4.1. The min-\# optimal version from [4] has different complexities depending on whether the polyline is open or closed (as well as convex vs. concave). The authors have not properly brought this in context.}

Yes, we have revised the second paragraph of section 3 to clarify this. Thanks for pointing out this!

\textbf{[R3C1b]} \emph{Also, in multiple places, the authors are bringing the notion of the "3D" - however, they do not mention the reference [r1] which was the first one to extend the algorithm in [4] to 3D. }

\emph{[r1] Gill Barequet, Danny Z. Chen, Ovidiu Daescu, Michael T. Goodrich, and Jack Snoeyink. Efficiently approximating polygonal paths in three and higher dimensions. Algorithmica, 33(2):150–167, 2002.}

Yes, we have clarified this in the first paragraph of Section 4.3.3. Thanks!

\textbf{[R3C2]} \emph{
Sec. 4.2. One aspect that is context-dependent (and often implicitly ignored) in online algorithms, is the communication overhead (which, in turn, may imply energy consumption in sensor networks); along with the complementary aspect of "freshness" of the compressed data in the respective sink. Please see [r2] below.}

\emph{[r2] Oliviu Ghica, Goce Trajcevski, Ouri Wolfson, Ugo Buy, Peter Scheuermann, Fan Zhou, Dennis Vaccaro: Trajectory Data Reduction in Wireless Sensor Networks. IJNGC 1(1) (2010)
}

Yes, we have added this in the first paragraph of Section 4.2. Thanks!

\textbf{[R3C3a]} \emph{
Sec. 4.3. Actually [13] does provide a kind of a taxonomy, and it would be nice to compare it with the present article. }

We have briefly compared them in the 3rd paragraph of Section 1 (Algorithm taxonomy). Indeed, they are identical. Thanks for your advice!

\textbf{[R3C3b]} \emph{Also, [37] has a claim/proof that if the online version uses $\varepsilon$ as an error-bound in the LDR, then the final outcome is an equivalent to a compression of $2 \cdot \varepsilon$ applied to the entire historic trajectory (if it were available). This, in a sense, is a form of a justification  for the use of $\varepsilon$/2 in works like CISED ([14]).}

Yes, we have stated this in the 5th paragraph of Section 4.3. Thanks for pointing out this!

\textbf{[R3C4]} \emph{
In several places, the authors are making statements that are not properly phrased, and some that can be taken out of context. Examples:}

\emph{1. The authors should try to define terms before their first use. Example: on p.1 in the Introduction, the ``min-\#" is brought up, without properly explaining its meaning.}


\emph{2. p.2: ``...no need of extra knowledge and suitable for freely moving objects [28]...". A statement of this sort does not do justice to [28].
Namely, [28] considers what happens with compression if the trajectories are constrained to move along road networks - but, it also is based on the motivation that in such settings there may be a different perspective - i.e., the overall-compression of the DB.}



\emph{3. p.33: the authors make a claim to the effect of: ``These methods lack the capability of compressing..." when describing the min-$\epsilon$ variant. The statement needs to be clarified, or put in the more appropriate contexts: the min-$\epsilon$  is a complementary one to min-\# and, as such, it never makes any other claim that it will yield the minimum error, given a fixed ``budget" of m points. However, that does not mean that this variant of the problem, and the algorithmic solution, doesn't have its own merits.}

We have revised the second paragraph of Section 1 with respect to the above comments. Thanks for your advice!

\line(1,0){500}



Your sincerely,

Anonymous authors
%Xuelian Lin, Shuai Ma, Yanchen Hou, Yihao Fu and Tianyu Wo

%\bibliographystyle{abbrv}
%\bibliography{sec-ref}


\end{document}
