\section{Latent Factor Model for Link Prediction}
\label{sec-NMF}
\marked{In this section, we first provide a link prediction method 
based on latent factor models. We then design an efficient method to 
search the possible links from $O(n^2)$ search space.}


We assume that $G=(N, A)$ is an undirected network (or graph) containing node set $N$ and
edge set $A$. The node set $N$ contains $n$ nodes and the edge set
$A$ contains $m$ edges. Furthermore, the $n \times n$ weight matrix
$W= [w_{ij}]_{n\times n}$ contains the weights of the edges in $A$.
The weight matrix is useful in representing the strengths of network
connections in  many real-world settings such as the number of
publications between a pair of co-authors in a bibliographic
network.  The matrix is sparse, and many its entries are 0, which could be
interpreted either as absence of links or as missing entries. While
we assume that an undirected network is available, the approach can
also be generalized to directed networks.
%
 The top-$k$ ranking problem for link prediction is formally stated as follows:

\begin{definition}
Given a network $G=(N, A)$ with node set $N$ and edge set $A$, the ranking problem for link
prediction is to determine the top-$k$ node-pair recommendations such that these node pairs are not included in $A$.
\end{definition}
Note that this problem definition requires us to consider the entire
search space of $O(n^2)$ possibilities, rather than a sample of the
node pairs in the network.

Latent factor models work by associating a low dimensional factor
with each row and column of the network. However, since link
prediction is (predominantly) studied only for undirected networks,
which have symmetric weight matrices, it suffices to associate an
$r$-dimensional latent factor $\overline{f_i}$ with the $i$th node
in the network. The value of $r$ is the {\em rank} of the
factorization. This is an issue, which we will discuss slightly
later.  The weight of a link between nodes $i$ and $j$ is defined by
the use of the dot product between the factors of nodes $i$ and $j$.
In other words, for the weight matrix $W= [w_{ij}]_{n\times n}$, we
would like to achieve the following:
\begin{equation}
w_{ij} \approx \overline{f_i} \cdot \overline{f_j}, \ \ \forall i, j
\in \{ 1, \ldots, n \} \label{factor}.
\end{equation}
This condition can be directly written in the matrix form. Let $F$ be an
$n \times r$ matrix, in which the $i$th row is the row vector
$\overline{f_i}$. Then, the above condition of
Equation~(\ref{factor}) can be written as follows:
\begin{equation}
W \approx F F^T.
\end{equation}


%Note that the diagonal entries of $W$ are 0, whereas any reasonable
%factorization $F F^T$ will always have non-zero diagonal entries
%with magnitudes  dependent on the node degrees.  To make the
%factorization more robust, we set each diagonal entry $w_{ii}$ to
%the sum of the weights of the edges  incident on it.
%\begin{equation}
%w_{ii}= \sum_{j \not=i} w_{ij}
%\end{equation}
%%The resulting matrix can  also be shown to be  positive
%%semi-definite, which ensures that an {\em exact} rank-$n$
%%factorization of $W$ into the form $F F^T$ always exists.
%The resulting matrix can also be shown to be nonnegative
%diagonally dominant symmetric, which ensures that an {\em exact}
%rank-$n$ factorization of $W$ into the form $F F^T$ always exists \cite{berman}.
%Of course, we are only interested in a rank-$r$ factorization ($r << n$),
%corresponding to dominant latent components,  because such a
%factorization helps in discovering {\em latent} edges.


An important question arises as to whether entries in the matrix $W$
corresponding to the absence of links should be treated as incomplete
entries or whether they should be treated as zero, with the possibility
of being incorrect. When latent factor models are used in
collaborative filtering, such entries are typically treated as
missing entries. However, unlike the absence of a rating, the
absence of a link is indeed useful information {\em in the
aggregate}, even though some node pairs have the propensity to form
links {\em in the future}. Therefore, we argue that, unlike
collaborative filtering, $W$ should be treated as a completely
specified matrix, but with noisy entries. Therefore, in the link
prediction problem, latent factor models should be viewed as a way
of {\em correcting} noisy entries with zero values, rather than
strictly as a missing value estimation problem.  Such assumptions
also simplify the algorithmic development of latent factor models
for link prediction. The idea here is that when we approximately
factorize $W$ into the form  $F F^T$, the positive values of entries in
$F F^T$ can be viewed as the {\em predictions} of  noisy 0-entries in
$W$.

A second important question arises as to the choice of the latent
factor model that must be used for prediction. There are many
choices available for factorizing an adjacency matrix, especially
when it is symmetric. Even a
straightforward diagonalization of the matrix provides a reasonable
factorization. We  choose {\em non-negative} matrix factorization (\NMF)
not only because of its interpretability advantages but also because
it facilitates the  $O(n^2)$ search phase of the prediction by
providing a non-negative and sparse representation for each node.

We would like to determine the matrix $F$ such that the Frobenius
norm of $( W - F F^T)$ is minimized.  This problem is referred to as
symmetric \NMF, and an efficient solution is proposed in
\cite{long}, where $F$ can be determined by starting with random
nonnegative entries in $(0, 1)$, and using the following
multiplicative update rule:
% {\bf Charu Note: Please check that this update works during implementation. NMF is sometimes unpredictable.}
%\begin{equation}
%F_{ij} \leftarrow  F_{ij} \frac{(W F)_{ij} }{(F F^T F)_{ij}}
%\end{equation}
\begin{equation}
\label{equation-update}
F_{ij} \leftarrow F_{ij} \left( 1 - \beta + \beta \frac{ \left(W F \right)_{ij} }{ \left(F F^T F \right)_{ij}} \right),
\end{equation}
in which $\beta$ is a constant in $( 0, 1]$ \cite{ding}.

\stitle{Discussions of computational complexity}. Let us examine the computational complexity of
the update Equation~(\ref{equation-update}).
The matrix $F F^T F$  can be fully materialized  with
$O(r^2 \cdot n)$ matrix multiplications, and the matrix $W F$ can be
computed in $O(m \cdot r)$ multiplications  by observing that the
sparse matrix $W$ has only $2m$ non-zero entries corresponding to the
number of edges. Therefore, each update takes $O(n
\cdot r^2 +m\cdot r )$ time.

This remains quite high for large networks, which motivates us to develop fast searching techniques to speed up the process.


\subsection{Efficient Top-$K$ Prediction Searching}
\label{sec-NMF-topk}

An advantage of the nonnegative factorization approach is that it
enables an efficient search phase, which is generally not possible
with most other link prediction methods. The value of
$\overline{f_i} \cdot \overline{f_j}$ in Equation~(\ref{factor}) provides a prediction value
for the links. The goal of the search phase is to return the top-$k$
links with the largest prediction values. In real-world settings, the matrix $F$ is
often nonnegative and sparse \cite{NMF-nature99}. This non-negativity and sparsity are
particularly useful in enabling an efficient approach. In order to
speed up the search, we define the notion of $\epsilon$-approximate
top-$k$ predictions, denoted as top-$(\epsilon, k)$ predictions.


\begin{definition}[top-$(\epsilon, k)$ predictions]
%A set $S$ of predicted links is a top-$(\epsilon, k)$ prediction, if
%the cardinality of $S$ is $k$, and the  $k$th best  value of
%$\overline{F_i} \cdot \overline{F_j}$ for a  link $(i, j) \in S$ is
%at most $\epsilon$ less than the   $k$th best value of
%$\overline{F_k} \cdot \overline{F_k}$ over any edge $(k , l)$ in the
%network.
A set $L$ of predicted links in a network is a top-$(\epsilon, k)$ prediction, if
the cardinality of $L$ is $k$, and the $k$-th best  value of
$\overline{f_i} \cdot \overline{f_j} $ for a link $(i, j) \in L$ is
at most $\epsilon$ less than the $k$-th best value of
$\overline{f_h} \cdot \overline{f_l}$ over any link $(h , l)$ in the
network.
\end{definition}
Intuitively, this definition allows a qualitative  tolerance of
$\epsilon$ in the top-$k$ returned links. Allowing such a tolerance
significantly helps in speeding up the search process by pruning
large portions of the search space, which is particularly important
in an $O(n^2)$ search space of link predictions.

The first step is to create a new $n \times r$ matrix $S$,
obtained by sorting the columns of $F$ in a descending order. An inverted list is
associated with each of the $r$ latent variables containing the node
identifiers  of $F$ arranged according to the sorted order of $S$. The
$r$ inverted lists can also be represented as an $n \times r$ matrix
$R$. Let the number of rows in the $p$-th column of $S$ ($p\in[1, r]$), for which
the value of $S_{ip}$ is greater than $\sqrt{\epsilon/r}$ be $f_p$, and for which the value of $S_{ip}$ is greater than 0 be $f'_p$, respectively.

Then the following nested loop is executed for each (say $p$-th) column of $S$:

%\vspace{-1ex}
\begin{tabbing}1. \hspace{5ex}\=
{\bf for} \= each $i=1$ to $f_p$ {\bf do}\\
2. \>\>{\bf for} \= each $j=i+1$ to $f'_p$ {\bf do}\\
3. \>\>\>{\bf if}\ \= $S_{ip} \cdot S_{jp} < \epsilon/r$ {\bf then}\\
4. \>\>\>\>break inner loop; \\
5. \>\>\> {\bf else} increase the score of node-pair $(R_{ip}, R_{jp})$ by \\
   \>\>\> \ \ \ \ \ \ an amount of $S_{ip} \cdot S_{jp}$\\
6. \>\> {\bf endfor}\\
7. \>{\bf endfor}
\end{tabbing}
%\vspace{-1ex}

This nested loop is designed to track the relevant subset of node
pairs from which one can determine the top-$(\epsilon, k)$
predictions.  The nested loop typically requires much less time than
$O(n^2)$ time because large portions of the search space are pruned.
First, depending on the value of $\epsilon$, the value of $f_p$ is
much less than $n$.  This is particularly true if many entries of
the factorized matrix $F$ are close to 0.  Furthermore, the inner
loop is often terminated early. The value of $\epsilon$ therefore
provides the user a way to set the tradeoff between accuracy and
efficiency. A hash-table is maintained which tracks all the pairs
$(R_{ip}, R_{jp})$ encountered so far in the nested loop. Because of
the pruning, the hash table usually needs to track a miniscule set
of the $O(n^2)$ node-pairs in order to determine the ones that truly satisfy the top-$(\epsilon, k)$ requirement. In the process, we exclude
the links which have already been represented with non-zero entries in $W$
because such links are always likely to have the largest prediction values,
which further reduces the searching space.

\eat{%%%%%%EAT
\begin{lemma}
For a $n \times r$ matrix $F$, the complexity of the top-$(\epsilon, k)$
prediction is $O(nn')$, where $n'$ is the number of entries of $F$
which are greater than $\sqrt{ \epsilon / r}$.
\end{lemma}
\begin{IEEEproof}
For the $p$th column of $F$, the {\bf if} statement in the nested loop
is executed $f_p ( 2n_p - f_p -1) / 2$ times. There are $r$ columns in $F$,
hence, the {\bf if} statement will be executed
$\sum_{p = 1}^{r} f_p ( 2n_p - f_p -1) / 2 $ times.
$\sum_{p = 1}^{r} f_p ( 2n_p - f_p -1) / 2 \leq n\sum_{p = 1}^{r}f_{p}$, and
$\sum_{p = 1}^{r} f_p$ is equal to the number $n'$. Therefore, the complexity of
top-$(\epsilon, k)$ prediction is $O(nn')$.
\end{IEEEproof} }

 It remains to show that this procedure does find a valid set of top-$(\epsilon,
k)$ link predictions. The reason that the procedure works correctly
and efficiently  is because of nonnegativity and sparsity. \marked{We next use an
example to illustrate how to search the top-$(\epsilon, k)$ link predictions based
on the above nested loop method.}

%%%%%%%%%%% example for top-k prediction
\vspace{2ex}
\noindent\marked{\begin{example}
\label{ex_top_k}
Given a $5 \times 3$ matrix $F$, we sort the three columns of $F$ in a
descending order to generate the matrix $S$ and store the corresponding
inverted indices in the matrix $R$, shown as follows:
\[\underbrace{
  \begin{bmatrix}
    0.7 & 0.3 & 0.7 \\
    0.5 & 0.7 & 0.9 \\
    0.4 & 1.1 & 0.7 \\
    0.0 & 0.8 & 0.0 \\
    0.5 & 0.0 & 0.1
  \end{bmatrix}}_{F}
  \Longrightarrow
  \underbrace{\begin{bmatrix}
    0.7 & 1.1 & 0.9 \\
    0.5 & 0.8 & 0.7 \\
    0.5 & 0.7 & 0.7 \\
    0.4 & 0.3 & 0.1 \\
    0.0 & 0.0 & 0.0
  \end{bmatrix}}_{S}
  \underbrace{\begin{bmatrix}
    1 & 3 & 2 \\
    2 & 4 & 1 \\
    5 & 2 & 3 \\
    3 & 1 & 5 \\
    4 & 5 & 4
  \end{bmatrix}}_{R}
\]
Assume that $\epsilon = 1$, then $\sqrt{ \epsilon / r} = 0.58$ and $\epsilon / r = 0.33$.
For $p = 1$ (resp. $p = 2$ and $p = 3$), we have ($f_1 = 1, f'_1 = 4$)
(resp. ($f_2 = 3, f'_2 = 4$) and ($f_3 = 3, f'_3 = 4$)) and only need to calculate
9 multiplications: ($S_{11} \cdot S_{21}$,
$S_{11} \cdot S_{31}$) (resp. ($S_{12} \cdot S_{22}$, $S_{12} \cdot S_{32}$, $S_{12} \cdot S_{42}$,
$S_{22} \cdot S_{32}$) and ($S_{13} \cdot S_{23}$, $S_{13} \cdot S_{33}$, $S_{23} \cdot S_{33}$))
rather than 75 multiplications for calculating $FF^T$ directly, thus pruning a large portion of
the search space. We then save the above values into corresponding node pairs,
\eg increase the score of node pair ($R_{11}, R_{21}$) by $S_{11} \cdot S_{21}$,
and finally return all node pairs that we encountered.
\end{example} }



\begin{prop}
\label{prop-valid-topk}
The nested loop method finds a valid set of top-$(\epsilon, k)$
predictions.
\end{prop}
\begin{IEEEproof}
The main part of the proof is to show that any dot product is
underestimated by at most  $\epsilon$. The aforementioned pseudo-code
containing the  nested loop is executed $r$ times, once for each
latent component. Therefore, it suffices to show that the
contribution of each nested loop is underestimated by at most
$\epsilon/r$. There are two sources of underestimation:
\begin{enumerate}
\item  The outer loop does not consider rows $i$ for which $S_{ip} <
\sqrt{\epsilon/r}$. This effectively prunes the products between
pairs $(i, j)$ for which both $S_{ip}$ and $S_{jp}$ are less
than $\sqrt{\epsilon/r}$ \marked{ since the matrix $S$ is sorted and $S_{ip} \geq S_{jp}$}.
Therefore, the underestimation because of
the ignoring of this pair is at most $\sqrt{\epsilon/r} \times
\sqrt{\epsilon/r} = \epsilon/r$.
\item The second case is when the inner loop is terminated early.
The early termination condition in this case is that the product is
at most $\epsilon/r$.
\end{enumerate}
Therefore, in both these mutually exclusive cases, the
underestimation is at most $\epsilon/r$. Therefore, over all latent
components the aggregate underestimation is at most
$(\epsilon/r)\times r= \epsilon$.

This completes the proof.
\end{IEEEproof}

\subsection{Speeding up Top-$(\epsilon, k)$ Predictions}
\label{sec-NMF-topk-optimization}

%We next propose an optimization for speeding up top-$(\epsilon, k)$ predictions.

 In real-life applications,
a common scenario is to find a set of top-$(\epsilon, k)$ predictions with such that the prediction
values are required to be greater than a given threshold $\theta$ \cite{ballard2015, lemp}. In this case,
the top-$(\epsilon, k)$ prediction could be speeded up by pruning the node pairs
whose prediction values are equal to or less than $\theta$ \marked{(See more details about setting $\theta$ in Section 3.6)}.

The simple yet effective strategy is that the prediction value of a node pair $(i, j)$ satisfies:
\begin{equation}
\label{equation-length}
\overline{f_i} \cdot \overline{f_j} = \|\overline{f_i} \| \cdot \| \overline{f_j} \| \cdot
\cos(\overline{f_i}, \overline{f_j}) \leq \|\overline{f_i} \| \cdot \| \overline{f_j} \|,
\end{equation} in which $\|\cdot\|$ is the length of a vector.

By Equation \ref{equation-length}, we obtain the following:
\begin{equation}
\label{equation-length-pruning}
\|\overline{f_i} \| \cdot \| \overline{f_j} \| \leq \theta \Longrightarrow \overline{f_i} \cdot \overline{f_j} \leq \theta
\end{equation}
The prediction value is no larger than $\theta$ if the product of the vector lengths  is no larger than $\theta$.
As we are to find the node pairs whose prediction values are greater than $\theta$, we can ignore
the node pairs whose length products are no larger than $\theta$ in the search process.

After  the matrixes $S$ and $R$ are obtained, an additional step is required to calculate the length of each row
vector $\overline{f_i}$ and store them in an array $M$. Then the following if-clause is inserted  into line 3
in the above nested loop to prune the node pairs whose prediction values are equal to or less than $\theta$.
\begin{tabbing}\hspace{5ex}\=
{\bf if} $M[R_{ip}] \cdot M[R_{jp}] \leq \theta$ {\bf then} continue;
\end{tabbing}


%It is intuitive that this procedure correctly finds a valid set of top-$(\epsilon, k)$
%predictions with prediction values are greater than $\theta$. Since the node pairs whose
%length products are less and equal than $\theta$ have been ignored in the nested loop,
%this procedure speeds up the nested loop method.

\begin{prop}
The top-$(\epsilon, k)$ prediction with speeding up technique finds a valid set of top-$(\epsilon, k)$
predictions with prediction values are greater than $\theta$.
\end{prop}
\begin{IEEEproof}
By Proposition~\ref{prop-valid-topk}, the nested loop finds a valid set of
top-$(\epsilon, k)$ predictions. The difference between the nested loop
and the optimized nested loop (\ie the top-$(\epsilon, k)$ prediction with speeding up technique)
is that the latter does not consider the node pairs $(i, j)$
when $\|\overline{f_i} \| \cdot \| \overline{f_j} \| \leq \theta$.
By Equation~(\ref{equation-length-pruning}), the prediction values of those node pairs
are also  equal to or less than $\theta$.

This completes the proof.
\end{IEEEproof}
%%%%%%%%%%%%%%%%%%%%%%   2016-06-20 duan end


%\vspace{-2ex}
\stitle{Discussions}. While the basic matrix factorization method is able to allow us to provide
efficiency and pruning to the search process, it is still not quite
as fast as one may need for large networks. The main problem
arises as a result of the factorization process itself, which can
require as much as $O(r \cdot (m + n\cdot r))$.  Typically, the
number $r$ of latent factors varies from the orders of a few ten to a few hundred~\cite{NMF-nature99, NMF-www2010}. For
sparse networks, whose node degrees are less than $r$, the $O(n
r^2)$ term might be the bottleneck.  The required  number of latent
components $r$ is often expected to increase with the network size. In
order to handle this computational problem, we propose the method of
{\em ensemble decomposition} that provides both efficiency {\em
and} \marked{robustness} advantages.
