\documentclass{letter}
\usepackage{geometry}
\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
%\setlength{\parindent}{0pt}

\begin{document}

Prof. Xuemin Lin,\\
Editor-in-Chief,\\
IEEE Transactions on Knowledge and Data Engineering\\

Dear Prof. Lin,

Attached please find a revised version of our submission to
IEEE Transactions on Knowledge and Data Engineering, \emph{An Ensemble
Approach to Link Prediction}.

The paper has been substantially revised according to the referees¡¯ comments.
In particular, (1) we have added discussions on xxx, (2) we have added the
discussions of xxx new references in the related work, and (3) we have also
taken this opportunity to rewrite several parts of the paper to improve the
presentation.

We would like to thank all the referees for their thorough reading of our
paper and for their valuable comments.

Below please find our responses to the comments by the referees.

%******************* reviewer 1 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Referee 1.}

\textbf{[R1C1]} \emph{When talking about the recommendation, in addition to
the survey by Adomavicius and Tuzhilin, I recommend another survey from
physical society [L. Lu, et al., Recommender Systems, Phys. Rep. 519 (2012) 1-49],
in particular, this survey discussed the similarity and difference of link
prediction and personalized recommendation.}

We added the recommended survey as a reference.



\textbf{[R1C2]} \emph{In a sparse network, for most common neighborhood
based methods (e.g., CN, AA, Resource Allocation Index, Jaccard coefficient, etc.),
the time complexity is not $O(n^2)$, but $O(n*k^2)$, where $k$ is the average degree.
It is because for most node pairs, the CN-based similarities are zero.
Therefore, the corresponding statements should be modified.}

xxx.


\textbf{[R1C3]} \emph{I can follow the operations to obtain S and R for
the efficient top-k search, however, the physical meaning of this approximation
is not clear. I strongly suggest the authors to show a clear picture,
for example, by illustrate the procedure for a small-size case link $n$ around 7,8
and r around 3,4. So that we could see the meaning of the method as well as
how to construct S and R step by step.}

xxx.


\textbf{[R1C4]} \emph{How to determine $\theta$ when using the speeding up
method (Eq. 4 and Eq. 5). In addition, I do NOT think the proof of
Proposition 2 is necessary, since it is very obvious and provides
nothing more than Eq. 4 and Eq. 5.}

xxx.


\textbf{[R1C5]} \emph{The edge bagging method is originally called snowball
sampling, please cite the original paper [P. Biernacki and D. Waldorf,
Snowball sampling: Problems and techniques of chain referral sampling,
Sociological Methods and Research 10 (1981) 141].}

xxx.


\textbf{[R1C6]} \emph{I personally like the biased edge bagging method,
and I agree with the statement "it often becomes more difficult to make
robust predictions between low-degree nodes". Please read and discuss
the following paper [Y. X. Zhu, Uncovering missing links with cold ends.
Physica A 391 (2012) 5769-5778.] that focuses on the related issue.
By the way, the authors are encouraged to test their novel bagging
methods for links with cold ends.}

xxx.


\textbf{[R1C7]} \emph{In addition to the related works from computer
science community, the authors should also highlight some important
works from physical science community, such as [Clauset, A., Moore, C., $\&$ Newman, M. E. (2008).
Hierarchical structure and the prediction of missing links in networks. Nature, 453(7191), 98-101],
[Guimer¨¤, R., $\&$ Sales-Pardo, M. (2009). Missing and spurious interactions
and the reconstruction of complex networks. PNAS, 106(52), 22073-22078]
and [L¨¹, L., Pan, L., Zhou, T., Zhang, Y. C., $\&$ Stanley, H. E. (2015).
Toward link predictability of complex networks. PNAS, 112(8), 2325-2330.].}

xxx.


\textbf{[R1C8]} \emph{Please compare the prediction accuracy of current
bagging methods with some other methods like resource allocation index
[Zhou, T., L¨¹, L., $\&$ Zhang, Y. C. (2009). Predicting missing links via
local information. The European Physical Journal B-Condensed Matter and
Complex Systems, 71(4), 623-630.], loop model [Pan, L., Zhou, T., L¨¹, L., $\&$ Hu, C. K. (2016).
Predicting missing links and identifying spurious links via likelihood
analysis. Scientific reports, 6, 22955.] and the above mentioned three
methods (see comment 7, Nature 2008, PNAS 2009 and PNAS 2015), in some
smaller size networks. Therefore we get more clear picture how accurate
the present method is.}

xxx.


%******************* reviewer 2 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Referee 2.}

\textbf{[R2W1]} \emph{Due to the fact that the decomposition is based
on random sampling, thus authors should provide the 95\% confidence
intervals of performance. This is important to demonstrate that the
proposed methods are significantly better than the state-of-the-art
methodology.}

xxx.

\textbf{[R2W2]} \emph{Authors are using top-k precision to evaluate
the link prediction results. However, the measurement is not used
correctly. Top-k precision will give a fair comparison between different
link prediction methods only when the K is well selected. Please refer
to the paper "Evaluating link prediction methods". Top-k precision
measurement can have different conclusions on link prediction results
when there are some small changes of the value K. K should be the
number of true links in test set (fixed). In this paper, authors
mentioned that the K range from $10^4 - 10^5$, this is not a correct
way to employ this measurement.}

xxx.

\textbf{[R2W3]} \emph{AA can only predict two hops links, it has no
predictive power to infer any potential links with larger hop distance.
Thus to provide a fair comparison, authors should provide the performance
comparison in each hop distance, namely hop 2, hop 3, and etc.
An example can be found in "Evaluating link prediction methods".}

xxx.


\textbf{[R2W4]} \emph{Authors proposed a framework to decompose the
large scale network link prediction problem, thus the latent factor
model can be replaced with some other link prediction methods. So
what if we replace latent factor model with the AA method, how is
the performance? Whether the ensembled AA results yield comparable
performance than the AA in the original network?}

xxx.


%******************* reviewer 3 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Referee 3.}

\textbf{[R3C1]} \emph{The section could be preceded by clearly noting
the two challenges (prediction/training complexity) in large networks,
and how separate tools are used to handle each.}

xxx.


\textbf{[R3C2]} \emph{The basic model proposed is $W = FF^T$. This is
quite sensible, but in comparison with the eigendecomposition,
one misses the diagonal matrix of possibly negative weights.
A comment on the limitation of this assumption may be prudent. See also
Peter Hoff. Modeling homophily and stochastic equivalence in symmetric
relational data. NIPS 2008.}

xxx.


\textbf{[R3C3]} \emph{The remark that absent links may be interpreted
as noisy ones is a reasonable one. There has been relevant work on
dealing with such noisy data in a matrix setting, e.g. Cho-Jui Hsieh,
Nagarajan Natarajan, Inderjit Dhillon. PU Learning for Matrix Completion.
ICML 2015. This is perhaps more prudent that directly fitting the value 0
for absent links.}

xxx.


\textbf{[R3C4]} \emph{A citation should be provided for the claim
that NMF usually results in sparse F. In standard matrix factorisation
models the latent weights are usually completely dense.}

xxx.


\textbf{[R3C5]} \emph{The explanation of the top-K prediction component
could be considerably improved. At present, the algorithm is presented,
and correctness shown, but one is not left with a lot of intuition for
what exactly is being done, and how it speeds up the computation. My
current understanding is that one views the computation of scores for a
pair (i, j) via
$W_{ij} = F_i F_j^T = S_i S_j^T = \sum_{p} S_{ip} * S_{jp} = \sum_{p} B^{p}_{ij}$
for a suitable set of matrices $B^{p}$. And that one then looks to compute
only some of the entries of this matrix, with the rest treated as zero.
However, in this view it is unclear why the loop over j does not include
the nodes with score greater than $\sqrt{(eps/r)}$; in fact what happens
when all nodes satisfy this condition is a bit unclear.}

xxx.

\textbf{[R3C6]} \emph{In Proof of Prop 1, it is worth commenting that
when we break the loop, we are guaranteed that $S(i,p) * S(j',p) < e/r$
for all further j', since the S are sorted.}

xxx.

\textbf{[R3C7]} \emph{In discussing the motivation, comment could be
made about the complexity of stochastic gradient training of latent
factor models.}

xxx.

\textbf{[R3C8]} \emph{Comment could be made that in principle, the
methods proposed here could be applied to any link prediction method,
and that there is nothing latent factor specific.}

xxx.


\textbf{[R3C9]} \emph{The Flickr dataset only seems to be used
in Sex 4.2.5, and not the other experiments. If this is indeed so,
there does not appear to be an explanation of why.}

xxx.


\textbf{[R3C10]} \emph{4.2.1 and 4.2.4 seem like they should be merged,
as both deal with the comparison of the same two groups of methods.}

xxx.

\textbf{[R3C11]} \emph{On the presentation side, below are a few
comments that may improve the final manuscript:\\
- perhaps using $\bar{f}_i$ rather $\bar{F}_i$ would be clearer\\
- use $\backslash$left( and $\backslash$right) for Eqn 3\\
- use $\cos$, $\log$, $\exp$\\
- Prop 3, "IS included"\\
- Sec 3.5, "pretty good result" is informal\\
- Sec 4, "impacts of various factors" seems vague\\
- Sec 4.1, "largest AA SCORING"\\
- Sec 4.1, "that THE more communities"\\
- Sec 4.1, "GIVEN F, the BIGCLAM"\\
- Sec 4, consider expanding lists of conclusions of the form (a) ..., (b) ..., so that each bullet starts on a new line.\\
- Exp 1.2, "methods SCALE better than"}

xxx.


\line(1,0){500}

Your sincerely,

Liang Duan, Charu Aggarwal, Shuai Ma, Tiejun Ma, Jinpeng Huai

\end{document}
