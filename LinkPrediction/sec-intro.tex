\section{Introduction}
\label{sec-intro}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.
The problem of {\em link prediction} or {\em link inference} is that
of predicting the formation of future links in a dynamic and
evolving network (see \cite{chancc,linyuan-2011,Hasan-2011} for surveys). The link prediction problem has numerous
applications, such as the recommendation of friends in a social
network, the recommendation of images in a multimedia network, or
the recommendation of collaborators in a scientific network, and, therefore, link
prediction methods have been studied extensively because of their numerous applications in various network-centered domains.

Link prediction methods are often applied to very large networks,
which are also sparse.  The massive sizes of such networks can
create challenges for the prediction process in spite of their
sparsity. This is because the {\em search space} for the link
prediction problem is of the size $O(n^2)$, where $n$ is the number
of nodes. Quadratic scalability can rapidly become untenable for
larger networks. In fact, an often overlooked fact is that most {\em
current link prediction algorithms evaluate the link
propensities only over a subset of possibilities rather than
exhaustively search for link propensities over the entire network, e.g.,
\cite{dwang,lee}}.
%
\eat{%%%%%%%%EAT
{\bf Charu Note: We do need to check this by looking at as many link
prediction papers as possible. At least the ones that I saw seemed
to have this property.}
}%%%%%%%%EAT
%
In order to understand why this is the case,
consider a network with $10^6$ nodes. Note that a size such as
$10^6$ is not large at all by modern standards, and even common
bibliographic networks such as \DBLP now exceed this size. Even
for this modest network, the number of {\em possibilities} for links
is of the order of $10^{12}$. Therefore, a 1GHz processor would
require at least $10^3$ seconds just to allocate one {\em machine cycle} to
every pair of nodes. This implies that in order to determine the
top-ranked link predictions over the {\em entire network}, the
running time will be much larger than $10^3$ seconds.  It is
instructive, therefore, to examine how this (lower bound on) running
time scales with increasing network size. Table~\ref{time} shows the
time requirements for allocating a single machine cycle to each
pair-wise possibility. The running times in this table represent
very optimistic lower bounds on the required times because link
prediction algorithms are complex and require far more than a single
machine cycle for processing a node-pair. Note that for larger
networks, even the presented lower bounds on the running times are
impractical.
\begin{table}
\caption{The $O(n^2)$ problem in link prediction: Time required to
allocate a {\em single machine cycle} to every node-pair possibility
in networks of varying sizes and processors of various speeds.}
\label{time}
\vspace{-2ex}
\centering
\begin{tabular}{cccc}
\hline \hline Network Sizes & 1 GHz &  3 GHz & 10 GHz \\
\hline \hline $10^6$ nodes & 1000 sec. & 333 sec. & 100 sec.\\
\hline $10^7$ nodes & 27.8 hrs &  9.3 hrs &  2.78 hrs\\
\hline $10^8$ nodes & $>100$ days &  $>35$ days & $> 10$ days\\
\hline $10^9$ nodes & $>10000$ days & $>3500$ days & $> 1000$ days\\
\hline \hline
\end{tabular}
\end{table}


%\begin{table}
%\vspace{4ex}
%{\small
%\begin{tabular}{cccc}
%\hline \hline Network Sizes & 1 GHz &  3 GHz & 10 GHz \\
%\hline \hline $10^6$ nodes & 1000 sec. & 333 sec. & 100 sec.\\
%\hline $10^7$ nodes & 27.8 hrs &  9.3 hrs &  2.78 hrs\\
%\hline $10^8$ nodes & $>100$ days &  $>35$ days & $> 10$ days\\
%\hline $10^9$ nodes & $>10000$ days & $>3500$ days & $> 1000$ days\\
%\hline \hline
%\end{tabular}}
%\vspace{-1ex}
%\caption{}
%\label{}
%\vspace{-3ex}
%\end{table}



It is noteworthy that most link prediction algorithms in the
literature are not designed to search over the entire space of
$O(n^2)$ possibilities. A closer examination of the relevant
publications shows that even for networks of modest size, these
algorithms perform benchmarking  only by evaluating over a {\em
sample of the possibilities} for links. This is only to be expected
in light of the lower bounds shown in Table~\ref{time}.  In other
words, the {\em complete ranking problem} for link prediction in
very large networks remains largely unsolved at least from a
computational point of view. It is evident from the presented lower
bounds in Table~\ref{time} that any ranking-based link prediction
algorithm {\em must integrate search space pruning} within the
prediction algorithm in order to even  have any  hope of exploring
the $O(n^2)$ search space in a reasonable amount of time. The
algorithmic design of most link prediction algorithms largely
overlooks this basic requirement~\cite{chancc,propflow}.

The link prediction algorithms are classified into unsupervised and
supervised methods. Unsupervised methods~\cite{kleinberg} typically
use neighborhood measures such as the Adamic-Adar~\cite{adamic}, and
the Jaccard coefficient. Supervised methods~\cite{propflow} treat
link prediction as a classification problem in which each node pair
is treated as a test instance. Supervised methods are the
state-of-the-art and generally provide more accurate results than
unsupervised methods~\cite{propflow}. It is also noteworthy that
most supervised methods evaluate link prediction algorithms by using
only {\em a sample} of test links because of computational
consideration.  In real-world applications, it is often desirable to
determine the {\em top-$k$} most relevant links for prediction {\em
over all possibilities for test links}. This problem remains largely
unsolved even for networks of any reasonable size.

The link prediction problem is also closely related to the missing value
estimation problem, which is commonly used in collaborative
filtering~\cite{adom}.  Just as collaborative filtering attempts to
predict missing entries in a matrix of users and items, the link
prediction problem attempts to predict missing entries in a
node-node adjacency matrix. In fact, the missing value estimation
framework seems to be a more compact and relevant model for the link
prediction problem, as compared with the vanilla classification
problem.  Many of the modern methods for collaborative filtering use
latent factor models~\cite{conceptualr,web} such as \SVD and
\NMF for predicting missing entries. These methods have been
shown to be wildly successful at least within the domain of
collaborative filtering~\cite{web}. In spite of the obvious
similarity between link prediction and collaborative filtering and
the obvious effectiveness of latent factor models, there are only a
few methods~\cite{menon} which attempt to use these models.

One of the reasons that latent factor models are rarely used in the
link prediction domain is simply because of their complexity. In
collaborative filtering applications, the item dimension is of the
order of a few hundred thousand, whereas even the smallest networks
in real-world settings contain more than a million nodes.
Furthermore, collaborative filtering methods  often perform the
recommendation on a per-user basis rather than try to determine
the top-$k$ user-item pairs.  The latter is particularly important
in the context of link prediction. The factorization of a matrix of
size $O(n^2)$ is not only computationally expensive, but also
memory-intensive.  As we will see later in this paper, one advantage
of  latent-factor models is that they are able to  transform the
adjacency matrix to a multidimensional space which can be searched
efficiently {\em by pruning} large portions of the $O(n^2)$ search
space in order to recommend the top-$k$ possibilities. 
\marked{Furthermore, more portions of the search space could be 
pruned when finding the top-$k$ possibilities that take values 
above some application-defined threshold \cite{ballard2015, lemp}. }
This is essential in such settings.

\stitle{Contributions}. In this paper, we explore an {\em ensemble-enabled approach} to
achieving the aforementioned goals.

We show how to make latent factor models
practical for link prediction by decomposing the search space into a
 set of smaller matrices. As a result, large parts of the $O(n^2)$
search space can be pruned without even having to evaluate the
relevant node pairs. \marked{An optimizing method is provided for 
speeding up the search process when a threshold is available.}
This provides an efficient approach for the
top-$k$ problem in link prediction.  Furthermore, our problem
decomposition method is an ensemble approach enabled with three 
structural bagging methods with performance guarantees, which has obvious {\em
effectiveness} advantages. \marked{Incorporating with some link prediction 
characteristics, the bagging methods maintain high prediction 
accuracy while reducing the network size via graph sampling techniques. }
Note that the same bias-variance
tradeoffs apply to the link-prediction problem, as they apply to the
standard classification problem. Therefore, the use of an ensemble
approach has obvious effectiveness advantages as well.

Using real-life datasets, we show that our ensemble-enabled approach for link prediction is both effective and efficient.
For instance, (1) on \Friendster with $15$ million nodes and $1$ billion edges, our approach could finish in an
hour, while direct \NMF, \Aa \cite{adamic} and \BIGCLAM \cite{yang-wsdm2013} could not finish in a day, and
(2) our approach improves the accuracy by $(18\%, 39\%, 33\%)$ (resp. $(4\%, 10\%, 18\%)$ and $(16\%, 11\%, 38\%)$ )
over direct \NMF, \Aa and \BIGCLAM on \YouTube, \Flickr and \Wikipedia, respectively.


\stitle{Organization}. This paper is organized as follows. In the next section, we  provide
the basic framework for the approach and describe the efficient use
of  latent factor models for link prediction. Section~3 discusses
how latent factor models can be augmented with ensembles to provide
more effective and efficient results. Section~4 presents and discusses the
experimental results, followed by related work in Section~5 and conclusions in Section~6.

