\documentclass{letter}
\usepackage{geometry}

% duan
\usepackage{xspace}
\usepackage{color}
\usepackage{amsfonts}

\newcommand{\marked}[1]{\textcolor{red}{#1}}

\newcommand{\kw}[1]{{\ensuremath {\mathsf{#1}}}\xspace}

\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}
\newcommand{\kwlog}{\emph{w.l.o.g.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\sstab}{\rule{0pt}{8pt}\\[-2.4ex]}

\newcommand{\topk}[1]{\kw{top}--\kw{#1}}
\newcommand{\topdown}{\kw{topDown}}
\newcommand{\extsubgraph}{\kw{compADS^+}}
\newcommand{\drfds}{\kw{FIDES^+}}
\newcommand{\extsubgraphold}{\kw{compADS}}
\newcommand{\findtimax}{\kw{maxTInterval}}
\newcommand{\findtimin}{\kw{minTInterval}}
\newcommand{\meden}{\kw{MEDEN}}

\newcommand{\tranformgraph}{\kw{convertAG}}
\newcommand{\mergecc}{\kw{strongMerging}}
\newcommand{\strongpruning}{\kw{strongPruning}}
\newcommand{\boundedprobing}{\kw{boundedProbing}}

\newcommand{\AFPR}{\kw{AFP}-\kw{reduction}}
\newcommand{\nwm}{{\sc nwm}\xspace}



\begin{document}



Prof. \marked{Ren√©e Miller} \\
Editor-in-Chief		\\
The VLDB Journal	\\



Dear Prof. Miller,

Attached please find a revised version of our submission to
the VLDB Journal, \emph{One-Pass Trajectory Simplification Using the Synchronous Euclidean Distance}.


The paper has been substantially revised according to the referees' comments. In particular, we have \marked{ (a) explained the technical decisions of our algorithms, (b) clarified the contributions that extend our original conference paper, and (c) refined the entire paper to improve its readability.}

We would like to thank all the referees for their thorough reading of our paper and for their valuable comments.

Below please find our detailed responses to the comments.


\line(1,0){500}

\textbf{Response to the comments of Associate Editor.}

\textbf{[AE]} \emph{The paper suffers from over-formalization that loses the intuition of the solution. In particular, the crucial sections 3 and 4 are too dense and fail to convey to readers the key technical ideas. The paper also contains numerous typos and grammatical errors. The revision should address these shortcomings. Consider providing an overview/outline of the approach first, rather than laying out a sequence of propositions. Moving some of the proofs to an appendix might be an option to improve readability. }




%******************* reviewer 1 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 1.}

\textbf{[R1C1]} \emph{On page 2, the first column is a little bit confusing. Going back and forth between SED and PED might confuse the readers. The second paragraph discusses optimal PED-based algorithms that run in $O(n^3)$ and $O(n^2)$. Then it says that the $O(n^2)$ algorithm is not applicable to SED distance. The third paragraph then describes some sub-optimal algorithms for SED-based simplification. {The fourth paragraph talks again about PED-based algorithms and mentions some linear-time algorithms but it's clear whether these algorithms are optimal or approximate. It is not clear either why do we need to reiterate over PED-based algorithms if they are not applicable to the addresses problem.}}
 
(I) Yes! In the fourth paragraph, the mentioned PED-based algorithms are approximate. 

(II) Here we reiterate over PED-based algorithms, our intention was to say that there are some PED-based linear-time algorithms but no SED-based linear-time algorithm yet, indeed, it is attractive to develop a SED-based linear-time algorithm. However, as you mentioned, this causes a bit confusing. So we rewrite the fourth and fifth paragraphs to clarify this.

\textbf{[R1C2]} \emph{Page 2 second column, it was a bit confusing what you mean by effective and efficient. I understood later that effective refers to the compression ratio and efficient refers to the running time. It would help to highlight this earlier or use clearer terms, e.g., faster instead of efficient.}

Yes, in the work, \emph{effective} refers to the compression ratio and \emph{efficient} refers to the running time. We have directly used compression ratio and running time instead of them. Thanks for raising this issue.

\textbf{[R1C3]} \emph{The last few paragraphs in Section 1 mention too many numbers that the reader cannot digest at that early point (all the comparison numbers to existing algorithms with different datasets.) I suggest mentioning only two or three numbers, e.g., the average speed up or the average compression ratio.}

Fixed. Thanks for pointing this out!

\textbf{[R1C4]} \emph{In Section 2.1, it is not clear whether the Directed line segment (L), its length, and angle are in the 2D or 3D space. I assume it is in the 2D space because the angle (theta) is measured in the 2D space (x,y) but you would better highlight this part.}

Yes, the length and the angle of a Directed line segment (L) are in the 2D space. We have highlight this in Section 2.1. Thanks for pointing this out!

\textbf{[R1C5]} \emph{In Section 3.1, the definition of spatio-temporal cones (C) mentions "w.r.t. a Point $P_s$" is this the starting point $P_s$ or any arbitrary point? If it is always the starting point $P_s$, then you might better rephrase it to "w.r.t. the starting point $P_s$."}

Yes, it is ``the" starting point and we have fixed it. Thanks for raising this issue.

\textbf{[R1C6]} \emph{In Section 3.4, it could be better if you mention the key idea of the proposed algorithm to help the readers understand it. For example, you can mention that the key observation is that every segment in the two polygons being intersected has to originate from one of the m edges of the regular polygon. Then you can mention how this helps skipping multiple segments at a time.}

Yes, we rewrite the first two paragraphs of Section 3.4 and highlight the key idea of the proposed Fast Regular Polygon Intersection algorithm. We hope this help readers to understand the algorithm. 
Thanks for your advice.

\textbf{[R1C7]} \emph{In the experiments section, Figures 12 and 16 are confusing as they compare the compression ratio and error of the proposed algorithm with and without the improved polygon intersection algorithm. It looks like the proposed improvement only affects the running time but produces the same output which makes it unclear how it might affect the quality of the results. It looks like the numbers in the two figures are exactly the same with both intersection algorithms (and they had to be the same). I suggest making two lines only CISED-S and CISED-W similar to figures 13-15 and 17-19.}

The motivation of our Fast Regular Polygon Intersection (RPolyInter) algorithm is that (1) it improves the running time of computing the common intersection of inscribed regular polygons, and at the same time, (2) it should have the same quality (compression ratio and error) as the Convex Polygon Intersection (CPolyInter) algorithm, \ie  given the same input, they output the same compressed trajectory.
According to this, the tests Exp 1.1, Exp 2.1 and Exp 3.1 are also designed to demonstrate: (1) our algorithm RPolyInter runs faster than algorithm CPolyInter (see Fig.20 and Fig.21), and (2) it has the same effetiveness (compression ratio and average error) as the later (see Fig.12 and Fig.16). 
%
Thus, we think it is beter to reserve the current lines as shown in Fig.12, Fig.16, Fig.20 and Fig.21, where ``R'' denotes using our algorithm RPolyInter and ``C'' denotes using algorithm CPolyInter.

\textbf{[R1C8]} \emph{For readability of the results, I suggest using consistent point types in all the figures. Currently, some point types are reused with different algorithms which can be confusing, e.g., CISED-S-C in Figure 12 is the same as CISED-W in Figure 13 and CISED-W-C in Figure 12 is the same as SQUISH-E in Figure 13.}

\textbf{[R1C9]} \emph{In Figure 14, it is not clear how the proposed CISED-W algorithm produced a compression ratio that is better than the optimal algorithm. You need to comment on this or correct it.}

We do not claim that ``the proposed CISED-W algorithm produced a compression ratio that is better than the optimal algorithm".
Actually, we find from Exp 1.3 and Fig.14 that algorithm CISED-W is comparable with the optimal algorithm, and is approximate $20\%$ better than algorithm DP using SED. 
Thanks!

\textbf{[R1C10]} \emph{The paper has some minor language issues that can be easily corrected with a proofread.\\
- ``an one" $\rightarrow$ ``a one" \\
- ``to fast the ..." $\rightarrow$ ``to speed up the ..." \\
- ``they forms a ..." $\rightarrow$ ``they form a ..."	\\
- ``in the executing of ..." $\rightarrow$ ``in the execution of ..."
}


Fixed! Thanks for pointing this out!

%******************* reviewer 2 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 2.}

\textbf{[R2C1]} \emph{Sections 3 and 4, which constitute the core of the paper, are written as a series of propositions, some of which are not needed or are not easy to grasp.
The authors could easily remove some and also add some intuition to others.}

Yes, we have elaborated these. Thanks for these nice suggestions!

(1) In Section 3.1, we have re-organized the contents into two parts. The first part defines the \emph{Synchronous Circles}, followed by Proposition 1 that shows the relationship between the \textit{synchronous circles} and the \textit{synchronous distances}; the second part defines the \textit{spatio-temporal cone}, followed by proposition 2 presenting the \textit{spatio-temporal cone intersection} method which converts the SED distance tolerance into the intersection of spatio-temporal cones.

(2) We have highlighted some key ideas or intuitions in Section 3 to help the readability of the method.

(3) Please refer to our responses to R2C6 and R2C7 for more details.

\textbf{[R2C2]} \emph{In terms of the setting of the problem, the authors did not discuss the device range error which varies from device to device and may affect the algorithms.
In addition, it seems that the authors assume that the devices move in uniform speed which may not be realistic.
}

(1) In this work, we study the lossy simplification of trajectory data after it is collected by mobile devices from GPS sensors. The quality of GPS data should not have obvious affects on line simplification algorithms due to the simply nature of line simplification approaches. Thus, we do not surpose the quality of GPS data, and the device range error is out of the scope of this work. 
%
We have also observed that there are many GPS device-level works focus on correcting GPS errors caused by clock inaccuracies, obstructions on GPS signal path, atmospheric effects and so on. Besides, map-matching is a method that maps trajectories on road paths and it is believed a way to correct the GPS error taking urban roads as the benchmark, and semantic-based trajectory simplification will first map trajectories on road paths and then remove redundant data points on road paths.  

(2) We do not assume that the devices move in uniform speed. Actually, our algorithm, like many other line simplification algorithms, is suitable for moving objects in any speeds and directions. It is also worth pointing out that in the spatio-temporal query, it assumes that \textit{the object is moving linearly in a uniform speed between two adjacent data points}. This assumption is also introduced to the discipline of trajectory simplification, \ie after a sub-trajectory $[P_s, ..., P_e]$ is represented by or simplified to a line segment $\overrightarrow{P_sP_e}$, the object is assumed moving from $P_s$ to $P_e$ linearly in a uniform speed.  

\textbf{[R2C3]} \emph{Now looking at some of the propositions.
The proof of proposition looks incorrect, take as an example a moving device along the y-axis only over the time axis. In this case, $p'_{s+i}.x - P_s.x = 0$ while $p'_{s+i}.y - P_s.y$ is not zero.
}

We have corrected the proof of \textbf{Proposition 1}. Thanks for raising this issue.


\textbf{[R2C4]} \emph{Except if I am missing something, projecting a cone over a plane is not necessarily a circle.}

No! The projection of a \emph{spatio-temporal cone} over a plane $P.t- t_c = 0$ ($t_c > P_s.t$) is certainly a circle. Please refer to the definiton of \emph{Spatio-temporal cone}, indeed, it is an oblique circular cone whose base is a  \emph{synchronous circle} on a plane being paralel to the plane $P.t- t_c = 0$. 

\textbf{[R2C5]} \emph{Looking at Proposition 4, if you take only two polygons with m edges, their intersection could have more than m edges. Or are the authors doing the intersection differently.}

We approximate a circle by a specialized polygon, \ie a fixed rotating and m-edges inscribed regular polygon, which ensures that the intersenction of these polygons has no more than $m$ edges. 

We have appended the first paragraph of Section 3.3 to clarify this. Thanks for raising this issue.

\textbf{[R2C6]} \emph{Propositions 6 and 7 are not really needed as they follow up from the way your algorithms function.}

No! Propositions 6 and 7 are derived from the definiton of inscribed regular polygon (Section 3.3) and the principles of the convex polygons intersecton algorithm (Section 2.4), and serve as the basis of the extra \emph{advance rules} that we apply to speed up the process of regular polygons intersection. 

We have rewrited the first two paragraphs of Section 3.4 to clarify this. Thanks!

\textbf{[R2C7]} \emph{Theorem 8 is not really needed.}

Yes! We have romoved it from the manuscript.

\textbf{[R2C8]} \emph{Referring to Exp-1.1, I wouldn't say that CISED-S is comparable to DPSED, a $~10\%$ difference is not small.}

\textbf{[R2C9]} \emph{In almost all the experiments, the behavior of the different algorithms across all datasets is quite similar; the authors should have commented on this.}

\textbf{[R2C10]} \emph{In the summary about the experiments, the authors should have discussed, based on compression ratios, average errors, and running time, which of the proposed algorithms should be used in practice under which circumstances.
In particular, looking at the average errors, how acceptable are these for the applications that will use the proposed compression algorithms. Being fast with high compression is good, but if the quality is not that good then no one will use these algorithms.
}

\textbf{[R2C11]} \emph{Few examples of the typos and the grammatical errors: \\
   a. In th abstract "algorithms have are been"	\\
   b. Abstract "comparable with and $19.6\%$" $\rightarrow$ The last sentence is a very hard to read.	\\
   c. Introduction (first paragraph) "devices have been using their sensors" $\rightarrow$ devices use their sensors	\\
   d. Page 2 -- Column 1 -- Last paragraph "to design an one-pass LS algorithm" \\
   e. Page 4 -- Column 1 -- "is no greater than" \\
   f. Page 9 -- Last paragraph -- "Algorithm FastRPolyInter. The presented" $\rightarrow$ it is the first time to talk about the algorithm so how it has been presented. \\
   g. Many single sentence paragraph throughout the paper. 
}

Fixed! Thanks!


%******************* reviewer 3 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 3.}

\textbf{[R3C1]} \emph{From the example in Fig. 1, it is really difficult to tell the difference between PED and SED. It seems that SED will use more line segments due to the given constraint enforced by epsilon. However, why is the compression using SED better than that using PED? If PED is good enough, why do we care about SED? Essentially, the motivation of leaning towards SED is never clear from a practical perspective. I think it all depends on the tolerance at the application level. So I would like the authors to evaluate some real applications on top of the compressed trajectory data, to demonstrate the benefits of using SED. At least, the authors should give some convincing arguments on that.}

(1) The meaning of SED is highly depending on the notion of the \emph{synchronized point}. Please refer to our response to \textbf{R3C2} for more detail about the \emph{synchronized point}. 

(2) When compressing trajectories, the choosing of PED or SED depends on the application retuirements. For example, a spatio-temporal query application, ``\emph{the position of a moving object at time $t$}", on trajectories compressed by algorithms using SED will return a point $P'$ whose distance to of the actual position of the moving object is within the bound $\epsilon$, while the same query on trajectories compressed by algorithms using PED will not. In a word, the using of PED brings good compression ratios as well as does not support applications like spatio-temporal query, while the using of SED is in contrast.

We have clarified these in the second and third paragraphs of Section 1. 
The formal definitions of the SED and PED are presented in Section 2.1.


\textbf{[R3C2]} \emph{The authors should give more intuitive explanation of the semantics of "synchronized points." Maybe this has been documented in the literature, but the authors should give sufficient explanation to make this paper self-contained.}

Intuitively, a synchronized data point $P'_i$, $s<i<e$, is a point on $\overrightarrow{P_sP_{e}}$ satisfying $\frac{|P_sP'_i|}{|P_sP_e|} = \frac{t_i - t_s}{t_e-t_s}$. 
Or we surppose that the moving object is moving from $P_s$ to $P_e$ at an average speed $\overline{v} = \frac{|P_sP_e|} {t_e-t_s}$, then its position at time $t_i$ on $\overrightarrow{P_sP_{e}}$ is $\overline{v}*t_i$ after $P_s$.
We have clarified this in the second and third paragraphs of Section 1. The formal definition of the synchronized data point is also presented in Section 2.1.
Thanks for pointing this out.


\textbf{[R3C3]} \emph{Examples 1, 2, 3, 4, and 5 are not illustrative: They basically explained what happened in the corresponding figures but did not explain why. So it is not helpful if readers try to understand the algorithms by reading these examples. More details should be given. For instance, in Example 1, you can give specific coordinates to certain points so that readers can verify how the SED points are computed.}

\textbf{[R3C4]} \emph{Following my first comment, I would suggest at least comparing with one algorithm based on PED (e.g., your previous work [15]). Also, I am not convinced by just using compression ratio as the single metric for measuring effectiveness. As I mentioned, I recommend using the compressed data to evaluate some real applications.}

(1) Yes, we have provided addtional tests that compare the performance of algorithms using PED VS. SED. Two pairs of algorithms are tested, the first is the algorithm Douglas-Peucker using PED and SED, respectively, and the second is the sector intersection algorithm using PED and our spatio-temporal cone intersection algorithm using SED. %Note that spatio-temporal cone intersection method is an extension of the sector intersection method.
The results are presented in the Appendix.

(2) We have observed that some \textcolor{red}{works} have evaluated trajectory simplification algorithms from the view of real applications, \eg spatio-temporal queries, however, we would not do this as we consider that there are so many important applications over trajectories and we are impossible to cover the most of them in this work. So, it is a good choice that we concentrate on designing the algorithm and testing the common metrics on the  algorithm, and let the extensive evaluations by other researchers or works.


\line(1,0){500}

Your sincerely,

Xuelian Lin, Jiahao Jiang, Shuai Ma, Yimeng Zuo and Chunming Hu

\end{document}
