\documentclass{letter}
\usepackage{geometry}

% duan
\usepackage{xspace}
\usepackage{color}
\usepackage{amsfonts}

\newcommand{\marked}[1]{\textcolor{red}{#1}}

\newcommand{\kw}[1]{{\ensuremath {\mathsf{#1}}}\xspace}

\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}
\newcommand{\kwlog}{\emph{w.l.o.g.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\sstab}{\rule{0pt}{8pt}\\[-2.4ex]}

\newcommand{\topk}[1]{\kw{top}--\kw{#1}}
\newcommand{\topdown}{\kw{topDown}}
\newcommand{\extsubgraph}{\kw{compADS^+}}
\newcommand{\drfds}{\kw{FIDES^+}}
\newcommand{\extsubgraphold}{\kw{compADS}}
\newcommand{\findtimax}{\kw{maxTInterval}}
\newcommand{\findtimin}{\kw{minTInterval}}
\newcommand{\meden}{\kw{MEDEN}}

\newcommand{\tranformgraph}{\kw{convertAG}}
\newcommand{\mergecc}{\kw{strongMerging}}
\newcommand{\strongpruning}{\kw{strongPruning}}
\newcommand{\boundedprobing}{\kw{boundedProbing}}

\newcommand{\AFPR}{\kw{AFP}-\kw{reduction}}
\newcommand{\nwm}{{\sc nwm}\xspace}


\newcommand{\cone}[1]{{$\mathcal{C}{#1}$}}
\renewcommand{\circle}[1]{{$\mathcal{O}{#1}$}}
\newcommand{\pcircle}[1]{{$\mathcal{O}^c{#1}$}}


\begin{document}



Prof. \marked{Ren√©e Miller} \\
Editor-in-Chief		\\
The VLDB Journal	\\



Dear Prof. Miller,

Attached please find a revised version of our submission to
the VLDB Journal, \emph{One-Pass Trajectory Simplification Using the Synchronous Euclidean Distance}.


The paper has been substantially revised according to the referees' comments. In particular, we have \marked{ (a) explained the technical decisions of our algorithms, (b) clarified the contributions that extend our original conference paper, and (c) refined the entire paper to improve its readability.}

We would like to thank all the referees for their thorough reading of our paper and for their valuable comments.

Below please find our detailed responses to the comments.


\line(1,0){500}

\textbf{Response to the comments of Associate Editor.}

\textbf{[AE]} \emph{The paper suffers from over-formalization that loses the intuition of the solution. In particular, the crucial sections 3 and 4 are too dense and fail to convey to readers the key technical ideas. The paper also contains numerous typos and grammatical errors. The revision should address these shortcomings. Consider providing an overview/outline of the approach first, rather than laying out a sequence of propositions. Moving some of the proofs to an appendix might be an option to improve readability. }

Yes, we have provided sufficient text to describe the intuitions and the key technical ideas of the solution. We have provided an overview/outline of the approach in the first paragraph of Section 3. Then in Section 3.1, we have re-organized the contents into two parts. The first part defines the \emph{Synchronous Circles}, followed by Proposition 1 that shows the relationship between the \textit{synchronous circles} and the \textit{synchronous distances}; the second part defines the \textit{spatio-temporal cone}, followed by proposition 2 presenting the \textit{spatio-temporal cone intersection} method. This re-organization makes the propositions be easy to follow. We have also highlighted some key ideas or intuitions in Sections 3.3 and 3.4 to help the readability of the method. 

We have fixed typos and grammatical errors raised by reviewers and carefully done times of proof-readings. And all comments from reviewers have been seriously solved and responsed. Thus, the quality and readability of the paper has been improved a lot.

%******************* reviewer 1 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 1.}

\textbf{[R1W1]} \emph{Some parts in the paper can be improved for readability.}

We have elaborated the paper. Please refer to our responses to R1C1, R1C2, R1C3, R1C4, R1C5 and R1C6 for more details. Thanks for your nice advices!

\textbf{[R1W2]} \emph{The experimental section needs some clarification on some points.}

We have clarified the issues you mentioned. Please refer to our responses to R1C7, R1C8 and R1C9 for more details. Thanks for your nice advices!

\textbf{[R1W3]} \emph{The paper has some minor grammatical mistakes.}

We have fixed these grammatical mistakes and carefully done times of proof-readings. Thanks again!

\line(1,0){100}

\textbf{[R1C1]} \emph{On page 2, the first column is a little bit confusing. Going back and forth between SED and PED might confuse the readers. The second paragraph discusses optimal PED-based algorithms that run in $O(n^3)$ and $O(n^2)$. Then it says that the $O(n^2)$ algorithm is not applicable to SED distance. The third paragraph then describes some sub-optimal algorithms for SED-based simplification. {The fourth paragraph talks again about PED-based algorithms and mentions some linear-time algorithms but it's clear whether these algorithms are optimal or approximate. It is not clear either why do we need to reiterate over PED-based algorithms if they are not applicable to the addresses problem.}}
 
(I) Yes! In the fourth paragraph, the mentioned PED-based algorithms are approximate. 

(II) For reiterating over PED-based algorithms, our intention was to say that there are some PED-based linear-time algorithms but no SED-based linear-time algorithm yet, indeed, it is attractive to develop a SED-based linear-time algorithm. However, as you mentioned, this causes a bit confusing. So we have rewritten the fourth and fifth paragraphs to clarify this.

\textbf{[R1C2]} \emph{Page 2 second column, it was a bit confusing what you mean by effective and efficient. I understood later that effective refers to the compression ratio and efficient refers to the running time. It would help to highlight this earlier or use clearer terms, e.g., faster instead of efficient.}

Yes, in the work, \emph{effective} refers to the compression ratio and \emph{efficient} refers to the running time. We have directly used compression ratio and running time instead of them. Thanks for raising this issue.

\textbf{[R1C3]} \emph{The last few paragraphs in Section 1 mention too many numbers that the reader cannot digest at that early point (all the comparison numbers to existing algorithms with different datasets.) I suggest mentioning only two or three numbers, e.g., the average speed up or the average compression ratio.}

Fixed. Thanks for pointing this out!

\textbf{[R1C4]} \emph{In Section 2.1, it is not clear whether the Directed line segment (L), its length, and angle are in the 2D or 3D space. I assume it is in the 2D space because the angle (theta) is measured in the 2D space (x,y) but you would better highlight this part.}

Yes, the length and the angle of a Directed line segment (L) are in the 2D space. We have highlight this in Section 2.1. Thanks for pointing this out!

\textbf{[R1C5]} \emph{In Section 3.1, the definition of spatio-temporal cones (C) mentions "w.r.t. a Point $P_s$" is this the starting point $P_s$ or any arbitrary point? If it is always the starting point $P_s$, then you might better rephrase it to "w.r.t. the starting point $P_s$."}

Yes, it is ``the" starting point and we have fixed it. Thanks for raising this issue.

\textbf{[R1C6]} \emph{In Section 3.4, it could be better if you mention the key idea of the proposed algorithm to help the readers understand it. For example, you can mention that the key observation is that every segment in the two polygons being intersected has to originate from one of the m edges of the regular polygon. Then you can mention how this helps skipping multiple segments at a time.}

Yes, we rewrite the first two paragraphs of Section 3.4 and highlight the key idea of the proposed Fast Regular Polygon Intersection algorithm. We hope this help readers to understand the algorithm. 
Thanks for your advice.

\textbf{[R1C7]} \emph{In the experiments section, Figures 12 and 16 are confusing as they compare the compression ratio and error of the proposed algorithm with and without the improved polygon intersection algorithm. It looks like the proposed improvement only affects the running time but produces the same output which makes it unclear how it might affect the quality of the results. It looks like the numbers in the two figures are exactly the same with both intersection algorithms (and they had to be the same). I suggest making two lines only CISED-S and CISED-W similar to figures 13-15 and 17-19.}

The motivation of developing the Fast Regular Polygon Intersection (RPolyInter) algorithm is that (1) it should improve the running time of computing the common intersection of inscribed regular polygons, and at the same time, (2) it should have the same quality (compression ratio and error) as the Convex Polygon Intersection (CPolyInter) algorithm, \ie  given the same input, they output the same compressed trajectory.
According to this, the tests Exp 1.1, Exp 2.1 and Exp 3.1 are also designed to demonstrate: (1) our algorithm RPolyInter runs faster than algorithm CPolyInter (see Fig.20 and Fig.21), and (2) it has the same effetiveness (compression ratio and average error) as the later (see Fig.12 and Fig.16). 
%
Thus, we think it is beter to reserve the current lines as shown in Fig.12, Fig.16, Fig.20 and Fig.21. %, where ``R'' denotes using our algorithm RPolyInter and ``C'' denotes using algorithm CPolyInter.

\textbf{[R1C8]} \emph{For readability of the results, I suggest using consistent point types in all the figures. Currently, some point types are reused with different algorithms which can be confusing, e.g., CISED-S-C in Figure 12 is the same as CISED-W in Figure 13 and CISED-W-C in Figure 12 is the same as SQUISH-E in Figure 13.}

We have used consistent point types in all the figures. Thanks for your nice advice.

\textbf{[R1C9]} \emph{In Figure 14, it is not clear how the proposed CISED-W algorithm produced a compression ratio that is better than the optimal algorithm. You need to comment on this or correct it.}

We do not claim that ``the proposed CISED-W algorithm produced a compression ratio that is better than the optimal algorithm".
Actually, we find from Exp 1.3 and Fig.14 that algorithm CISED-W is comparable with the optimal algorithm, and is approximate $20\%$ better than algorithm DP using SED. 
Thanks!

\textbf{[R1C10]} \emph{The paper has some minor language issues that can be easily corrected with a proofread.\\
- ``an one" $\rightarrow$ ``a one" \\
- ``to fast the ..." $\rightarrow$ ``to speed up the ..." \\
- ``they forms a ..." $\rightarrow$ ``they form a ..."	\\
- ``in the executing of ..." $\rightarrow$ ``in the execution of ..."
}


Fixed! Thanks for pointing this out!

%******************* reviewer 2 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 2.}

\textbf{[R2W1]} \emph{The contribution of the paper seems to be limited; the key idea is to approximate circles intersection with polygons intersection to determine when to start a new line segment in the summarized trajectory}

The contribution of the paper is not limited. Indeed, we have at least four important contributions, as follows:

(1) It is the first time presenting synchronized point and synchronous distances in a x-y-t 3D space and showing their properties.

(2) It is the first time providing a local distance checking method for trajectory compression using SED and proving its correctness.

(3) It is the first one-pass trajectory compression algorithm using SED, and it also has obvious better performance, both compression ratio and running time, than the most recent SED enabled trajectory compression algorithm SQUISH-E. 

(4) We also provide a method that finds the approximate common intersection of circles in a linear time. This method may have wide usages as the circle intersection is a basic functionality.

\textbf{[R2W2]} \emph{The paper is over formalized with many propositions, some of which may be incorrect.}

We have elaborated the paper. Please refer to our responses to R2C1, R2C2, R2C3, R2C4, R2C5, R2C6 and R2C7 for more details. Thanks for your nice advices!

\textbf{[R2W3]} \emph{There are too many typos and grammatical errors.}

We have fixed these grammatical mistakes and carefully done many times of proof-readings. Thanks again!

\line(1,0){100}

\textbf{[R2C1]} \emph{Sections 3 and 4, which constitute the core of the paper, are written as a series of propositions, some of which are not needed or are not easy to grasp. The authors could easily remove some and also add some intuition to others.}

Yes, we have elaborated these. Thanks for these nice suggestions!

(1) We have provided an overview/outline of the approach in the first paragraph of Section 3.

(2) In Section 3.1, we have re-organized the contents into two parts. The first part defines the \emph{Synchronous Circles}, followed by Proposition 1 that shows the relationship between the \textit{synchronous circles} and the \textit{synchronous distances}; the second part defines the \textit{spatio-temporal cone}, followed by proposition 2 presenting the \textit{spatio-temporal cone intersection} method which converts the SED distance tolerance into the intersection of spatio-temporal cones.

(3) We have highlighted some key ideas or intuitions in Sections 3.3 and 3.4 to help the readability of the method.


(4) Please refer to our responses to R2C6 and R2C7 for more details.

\textbf{[R2C2]} \emph{In terms of the setting of the problem, the authors did not discuss the device range error which varies from device to device and may affect the algorithms.
In addition, it seems that the authors assume that the devices move in uniform speed which may not be realistic.
}

(1) In this work, we study the lossy simplification of trajectory data after it is collected by mobile devices from GPS sensors. The quality of GPS data should not have obvious affects on line simplification algorithms due to the simply nature of line simplification approaches. Thus, we do not surpose the quality of GPS data, and the device range error is out of the scope of this work. 
%
We have also observed that there are many GPS device-level works focus on correcting GPS errors caused by clock inaccuracies, obstructions on GPS signal path, atmospheric effects and so on. Besides, map-matching is a method that maps trajectories on road paths and it is believed a way to correct the GPS error taking urban roads as the benchmark, and semantic-based trajectory simplification will first map trajectories on road paths and then remove redundant data points on road paths.  

(2) We do not assume that the devices move in uniform speed. Actually, our algorithm, like many other line simplification algorithms, is suitable for moving objects in any speeds and directions. It is also worth pointing out that in the spatio-temporal query, it assumes that \textit{the object is moving linearly in a uniform speed between two adjacent data points}. This assumption is also introduced to the discipline of trajectory simplification, \ie after a sub-trajectory $[P_s, ..., P_e]$ is represented by or simplified to a line segment $\overrightarrow{P_sP_e}$, the object is assumed moving from $P_s$ to $P_e$ linearly in a uniform speed.  

\textbf{[R2C3]} \emph{Now looking at some of the propositions.
The proof of proposition looks incorrect, take as an example a moving device along the y-axis only over the time axis. In this case, $p'_{s+i}.x - P_s.x = 0$ while $p'_{s+i}.y - P_s.y$ is not zero.
}

Yes, the proof of proposition.1 is not strict enough. In the preview version, we said:
``The intersection point $P'_{s+i}$ satisfies that $P'_{s+i}.t = P_{s+i}.t$ and
$\frac{P'_{s+i}.t - P_{s}.t}{Q.t - P_{s}.t}$ = $\frac{P_{s+i}.t - P_{s}.t}{Q.t - P_{s}.t}$  =
$\frac{|\overrightarrow{P_sP'_{s+i}}|}{|\overrightarrow{P_sQ}|}$ =
$\frac{P'_{s+i}.x - P_{s}.x}{Q.x - P_{s}.x}$ = 
$\frac{P'_{s+i}.y - P_{s}.y}{Q.y - P_{s}.y}$.
Hence, by the definition of synchronized points, we have the conclusion."
As you pointed out, it is possible that $|\overrightarrow{P_sQ}|=0$ or $Q.x - P_{s}.x=0$ or $Q.y - P_{s}.y$.

We have corrected the proof. The above proof is replaced by
``The intersection point $P'_{s+i}$ satisfies that 
(1) $P'_{s+i}.t = P_{s+i}.t$, 
(2) $P'_{s+i}.x$ = $P_s.x +  c\cdot(Q.x - P_s.x)$, and
(3) $P'_{s+i}.y$ = $P_s.y +  c\cdot(Q.y - P_s.y)$, 
where $c= \frac{P'_{s+i}.t - P_{s}.t}{Q.t - P_{s}.t}= \frac{P_{s+i}.t-P_s.t}{Q.t-P_s.t}$.
Hence, by the definition of synchronized points, we have the conclusion."

Thanks for raising this issue.


\textbf{[R2C4]} \emph{Except if I am missing something, projecting a cone over a plane is not necessarily a circle.}

No! The projection of a \emph{spatio-temporal cone} over a plane $P.t- t_c = 0$ ($t_c > P_s.t$) is certainly a circle. 

The synchronous circle of a data point $P_{s+i}$ \wrt an error bound $\epsilon$, denoted as \circle{(P_{s+i}, \epsilon)}, is \textcolor{blue}{a circle on the plane $P.t-P_{s+i}.t = 0$} such that $P_{s+i}$ is its center and $\epsilon$ is its radius.
%
Then, given the start point $P_s$ and an error bound $\epsilon$, the spatio-temporal cone of a data point $P_{s+i}$ \wrt the point $P_s$ and the error bound $\epsilon$, denoted as \cone{(P_s, \mathcal{O}(P_{s+i}, \epsilon))}, is \textcolor{blue}{an oblique circular cone} such that point $P_s$ is its apex and the synchronous circle $\mathcal{O}(P_{s+i}, \epsilon)$ of point $P_{s+i}$ is its base. Note that the base, \ie the synchronous circle $\mathcal{O}(P_{s+i}, \epsilon)$, is on the plane $P.t-P_{s+i}.t = 0$ which is sure parallel to any other plane $P.t- t_c = 0$. Hence, the  projection of a cone \cone{(P_s, \mathcal{O}(P_{s+i}, \epsilon))} on a plane $P.t- t_c = 0$ ($t_c > P_s.t$) is sure a circle.


\textbf{[R2C5]} \emph{Looking at Proposition 4, if you take only two polygons with m edges, their intersection could have more than m edges. Or are the authors doing the intersection differently.}

As you mentioned, the intersection of two \textcolor{blue}{general} $m$-edge polygons may produce a polygon with more than $m$ edges. To avoid this, we approximate a circle by a \textcolor{blue}{specialized} polygon, \ie a fixed rotating and m-edges inscribed regular polygon $\mathcal{R}(V, E)$,
where (1) $V=\{v_1, \ldots, v_{m}\}$ is the set of vertexes that are defined by a polar coordinate system, whose origin is the center $P$ of \pcircle{}, satisfying $v_j = (r, \frac{(j-1)}{m}2\pi), ~j \in [1, m]$
and (2) $E= \{\overrightarrow{v_mv_1}\} \bigcup \{\overrightarrow{v_jv_{j+1}}\ |\ j\in [1, m-1]\}$ is the set of edges that are labeled with the subscript of their start points.
This elaborate design ensures that the intersenction of these polygons has no more than $m$ edges, thus, we have Proposition 4.

We have appended the first paragraph of Section 3.3 to explain this. 

\textbf{[R2C6]} \emph{Propositions 6 and 7 are not really needed as they follow up from the way your algorithms function.}

No! Propositions 6 and 7 are derived from the definiton of inscribed regular polygon (Section 3.3) and the principles of the convex polygons intersecton algorithm (Section 2.4), and serve as the basis of the extra \emph{advance rules} that we apply to speed up the process of regular polygons intersection. 

We have rewrited the first two paragraphs of Section 3.4 to clarify this. Thanks!

\textbf{[R2C7]} \emph{Theorem 8 is not really needed.}

\textcolor{red}{Todo}
%Yes! We have romoved it from the manuscript.

\textbf{[R2C8]} \emph{Referring to Exp-1.1, I wouldn't say that CISED-S is comparable to DPSED, a $~10\%$ difference is not small.}

\textcolor{red}{Todo}

\textbf{[R2C9]} \emph{In almost all the experiments, the behavior of the different algorithms across all datasets is quite similar; the authors should have commented on this.}

Yes, we have added a summary (the first item) in Section 5.2.4. Thanks for pointing out this!

\textbf{[R2C10]} \emph{In the summary about the experiments, the authors should have discussed, based on compression ratios, average errors, and running time, which of the proposed algorithms should be used in practice under which circumstances. In particular, looking at the average errors, how acceptable are these for the applications that will use the proposed compression algorithms. Being fast with high compression is good, but if the quality is not that good then no one will use these algorithms.}

Yes, we have added a brief guideline for choosing an algorithm from the views of running time, compression ratio and average error. Please refer to Section 5.2.4 for more details. Thanks for your nice advice.

\textbf{[R2C11]} \emph{Few examples of the typos and the grammatical errors: \\
   a. In th abstract "algorithms have are been"	\\
   b. Abstract "comparable with and $19.6\%$" $\rightarrow$ The last sentence is a very hard to read.	\\
   c. Introduction (first paragraph) "devices have been using their sensors" $\rightarrow$ devices use their sensors	\\
   d. Page 2 -- Column 1 -- Last paragraph "to design an one-pass LS algorithm" \\
   e. Page 4 -- Column 1 -- "is no greater than" \\
   f. Page 9 -- Last paragraph -- "Algorithm FastRPolyInter. The presented" $\rightarrow$ it is the first time to talk about the algorithm so how it has been presented. \\
   g. Many single sentence paragraph throughout the paper. 
}

Fixed! Thanks!


%******************* reviewer 3 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 3.}

\textbf{[R3C1]} \emph{From the example in Fig. 1, it is really difficult to tell the difference between PED and SED. It seems that SED will use more line segments due to the given constraint enforced by epsilon. However, why is the compression using SED better than that using PED? If PED is good enough, why do we care about SED? Essentially, the motivation of leaning towards SED is never clear from a practical perspective. I think it all depends on the tolerance at the application level. So I would like the authors to evaluate some real applications on top of the compressed trajectory data, to demonstrate the benefits of using SED. At least, the authors should give some convincing arguments on that.}

(1) The meaning of SED is highly depending on the notion of the \emph{synchronized point}. Please refer to our response to \textbf{R3C2} for more detail about the \emph{synchronized point}. 

(2) When compressing trajectories, the choosing of PED or SED depends on the application retuirements. For example, a spatio-temporal query application, ``\emph{the position of a moving object at time $t$}", on trajectories compressed by algorithms using SED will return a point $P'$ whose distance to of the actual position of the moving object is within the bound $\epsilon$, while the same query on trajectories compressed by algorithms using PED will not. In a word, the using of PED brings good compression ratios as well as does not support applications like spatio-temporal query, while the using of SED is in contrast.

We have clarified these in the second and third paragraphs of Section 1. 
The formal definitions of the SED and PED are presented in Section 2.1.


\textbf{[R3C2]} \emph{The authors should give more intuitive explanation of the semantics of "synchronized points." Maybe this has been documented in the literature, but the authors should give sufficient explanation to make this paper self-contained.}

Intuitively, a synchronized data point $P'_i$, $s<i<e$, is a point on $\overrightarrow{P_sP_{e}}$ satisfying $\frac{|P_sP'_i|}{|P_sP_e|} = \frac{P_i.t - P_s.t}{P_e.t-P_s.t}$. 
Or we surppose that the moving object is moving from $P_s$ to $P_e$ at an average speed $\overline{v} = \frac{|P_sP_e|}{P_e.t-P_s.t}$, then its position at time $t_i$ on $\overrightarrow{P_sP_{e}}$ is a distance $\overline{v}*(P_i.t-P_s.t)$ after point $P_s$.

We have clarified this in the second and third paragraphs of Section 1. The formal definition of the synchronized data point is also presented in Section 2.1.
Thanks for pointing this out.

% [Meratnia2004] Nirvana Meratnia and Rolf A. de By. Spatiotemporal Compression Techniques for Moving Point Objects. EDBT, 2004.


\textbf{[R3C3]} \emph{Examples 1, 2, 3, 4, and 5 are not illustrative: They basically explained what happened in the corresponding figures but did not explain why. So it is not helpful if readers try to understand the algorithms by reading these examples. More details should be given. For instance, in Example 1, you can give specific coordinates to certain points so that readers can verify how the SED points are computed.}

Yes, we have add more detail for helping understand these examples. 

(1) As shown in the responses to R3C2, we have given intuitive explanation of the semantics of "synchronized points" which is helpful for understanding the computing of synchronized point and SED.

(2) We have added more details for Examples 1, 2, 3, 4, and 5. In Example 1, we have given coordinates to points and described the computing of a synchronized point. In Example 2, we have described the meaning of a circle in the Figure and its relation to a sector. In Example 3, we have explained how an \emph{advance rule} is actived so that edge $\overrightarrow{A}$ or $\overrightarrow{B}$ moves on a step. In Examples 4 and 5, we have redrawed the circles and added some comments in the captions.

\textbf{[R3C4]} \emph{Following my first comment, I would suggest at least comparing with one algorithm based on PED (e.g., your previous work [15]). Also, I am not convinced by just using compression ratio as the single metric for measuring effectiveness. As I mentioned, I recommend using the compressed data to evaluate some real applications.}

(1) Yes, we have provided addtional tests that compare the performance of algorithms using PED VS. SED. Two pairs of algorithms are tested, the first is the algorithm Douglas-Peucker using PED and SED, respectively, and the second is the sector intersection algorithm using PED and our spatio-temporal cone intersection algorithm using SED. %Note that spatio-temporal cone intersection method is an extension of the sector intersection method.
The results are presented in the Appendix.

(2) Firstly, the application requirements of the work directly comes from our CarStream [Zhang2017] project, an industrial data-processing system for chauffeured car services that has connected over 30,000 vehicles in more than 60 cities. In this project, trajectory data are collected from cars, transported to cloud, simplified/compressed and stored in databases, and then used for applications like tracking the historical trajectories of cars, analyzing the trips, mileages and travel patterns of cars, and so on. From these applications, we find that the metrics of compression ratio, max error and average error are enough to evaluate the qualify of our algorithm. Secondly, in the area of trajectory simplification, compression ratio, average error, max error and runing time of algorithm are the major metrics. We follow the customary practices. Finally, we have also observed that some {works}, eg. [Zhang2018], have evaluated trajectory simplification algorithms from the view of spatio-temporal queries, however, we would not do this as we consider that there are so many important applications over trajectories and we are impossible to cover the most of them in this work. So, it is a good choice that we concentrate on designing the algorithm and testing the common metrics on the  algorithm, and let the extensive evaluations by other researchers or works.

[Zhang2017] Mingming Zhang, Tianyu Wo, Tao Xie, Xuelian Lin and Yaxiao Liu. CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles. PVLDB, 10 (12), 2017.

[Zhang2018] Dongxiang Zhang, Mengting Ding, Dingyu Yang, Yi Liu, Ju Fan, and Heng Tao Shen. Trajectory Simplification: An Experimental Study and Quality Analysis. PVLDB, 11 (9): 934-946, 2018.

\line(1,0){500}

Your sincerely,

Xuelian Lin, Jiahao Jiang, Shuai Ma, Yimeng Zuo and Chunming Hu

\end{document}
