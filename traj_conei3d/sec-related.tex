%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-0.5ex}
\section{Related Work}
\label{sec-related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Trajectory compression algorithms are normally classified into two categories, namely lossless compression and lossy compression\cite{Muckell:Compression}.
(1) Lossless compression methods enable exact reconstruction of the original data from the compressed data without information loss.
%For example, delta compression \cite{Nibali:Trajic} is a lossless compression technique, which has zero error and a time complexity of $O(n)$, where $n$ is the number of data points in a trajectory. The limitation of lossless compression lies in that its compression ratio is relatively poor \cite{Nibali:Trajic}.
(2) In contrast, lossy compression methods allow errors or deviations, compared with the original trajectories.
These techniques typically identify important data points, and remove statistical redundant data points from a trajectory, or replace original data points in a trajectory with other places of interests, such as roads and shops. They focus on good compression ratios with acceptable errors.
%, and a large number of lossy trajectory compression techniques have been developed.
%
In this work, we focus on lossy compression of trajectory data, and  we next introduce the related work on lossy trajectory compression  from two aspects: line simplification based methods and semantics based methods.

\subsection{Line simplification based methods}


%Line simplification (\lsa) methods use a set of continuous line segments to represent a compressed trajectory (Fig.\ref{fig:notations}).
The idea of piece-wise line simplification comes from computational geometry.
Its target is to approximate a given finer piece-wise linear curve by another coarser piece-wise linear curve, which is typically a subset of the former, such that the maximum distance of the former to the later is bounded by a user specified bound $\epsilon$.
%
Initially, line simplification (\lsa) algorithms use perpendicular Euclidean distances (\ped) as the distance metric.
Then a new distance metric, the synchronous Euclidean distances (\sed), was developed after the \lsa algorithms were introduced to compress trajectories.
\sed was first introduced in the name of \emph{time-ratio distance} in~\cite{Meratnia:Spatiotemporal}, and formally presented in~\cite{Potamias:Sampling} as the \emph{synchronous Euclidean distance}.
\ped and \sed are two common  metrics adopted in trajectory simplification. The former usually brings better compression ratios while the later reserves temporal information in the result trajectories. Besides, there is direction based metric \cite{Long:Direction}  that preserves the directions of trajectories.



Line simplification algorithms can be classified into two aspects: compression optimal and sub-optimal methods.


\subsubsection{\textcolor{blue}{Compression} Optimal Algorithms}
For the ``min-\#" problem that finds out the minimal number of points or segments to represent the original polygonal lines \wrt an error bound $\epsilon$, Imai and Iri \cite{Imai:Optimal} first formulated it as a graph problem, and showed that it could be solved in  $O(n^3)$ time, where $n$ is the number of the original points.
%
Toussaint of \cite{Toussaint:Optimal} and Melkman and O'Rourke of \cite{Melkman:Optimal} improved the time complexity to $O(n^2 \log n)$ by using either \textit{convex hull} or \textit{sector intersection} methods.
%
The authors of \cite{Chan:Optimal} further proved that the \textcolor{blue}{compression} optimal algorithm using \ped could be implemented in $O(n^2)$ time by using the \textit{sector intersection} mechanism.
Because the \textit{sector intersection} and the \textit{convex hull} mechanisms can not work with \sed, hence, currently the time complexity of the  \textcolor{blue}{compression} algorithm using \sed remains $O(n^3)$.
That is, these \textcolor{blue}{compression} optimal algorithms are time-consuming and impractical for large trajectory data~\cite{Heckbert:Survey}.


\eat{A number of algorithms \cite{Agarwal:Metric, Chen:Fast, Wu:Graph} have been developed to solve the ``min-\#" problem under alternative error metrics.
\cite{Agarwal:Metric} studied the problem under the $L_1$ and uniform (also known as Chebyshev) metric. % while this study focuses on the $L_2$ metric.
 \cite{Chen:Fast} defined the local integral square synchronous Euclidean distance (LISSED) and the integral square synchronous Euclidean distance (ISSED), and used them to fast the construction of reachability graphs, and \cite{Wu:Graph} followed the ideas of \cite{Chen:Fast}.
{However, both LISSED and ISSED cumulative errors that leads to varied effectiveness compared with \sed.}
}
%
\eat{
The distinct Douglas-Peucker (\dpa) algorithm \cite{Douglas:Peucker} (see Figure~\ref{fig:notations}) is invented in 1970s, for reducing the number of points required to represent a digitized line or its caricature in the context of computer graphics and image processing.
The basic \dpa is a batch approach with a time complexity of $O(n^2)$.
%Algorithm \dpa is the foundation of many subsequent \lsa algorithms.
Some online \lsa algorithms were further developed by combining batch algorithms (\eg \dpa) with sliding/open windows \cite{Meratnia:Spatiotemporal, Liu:BQS}.
Recently, the authors of this paper developed a one-pass trajectory simplification algorithm (\operb) \cite{Lin:Operb}, which runs great fast and also has comparable compression ratio comparing with batch algorithms. Moreover, the ``cone intersection'' algorithm \cite{Williams:Longest, Sklansky:Cone, Dunham:Cone, Zhao:Sleeve} is another notable one-pass \lsa algorithm.
}


\subsubsection{\textcolor{blue}{Compression} Sub-optimal Algorithms}
Many studies have been targeting at finding the sub-optimal results.
%In this section, we focus on the piece-wise line simplification based methods for trajectory data.
In particular, the state-of-the-art of sub-optimal \lsa approaches fall into three categories, \ie batch, online and one-pass algorithms.
We next introduce these \lsa  based trajectory compression algorithms from three categories.

\stitle{Batch algorithms.}
The batch algorithms adopt a global distance checking policy that requires all trajectory points are loaded before compressing starts.
These batch algorithms can be either top-down or bottom-up.

Top-down algorithms, e.g., Ramer \cite{Ramer:Split} and Douglas-Peucker \cite{Douglas:Peucker}, recursively divide a trajectory into sub-trajectories until the stopping condition is met.
%
Bottom-up algorithms, e.g., Theo Pavlidis' algorithm \cite{Pavlidis:Segment}, is the natural complement of the top-down ones, which recursively merge adjacent sub-trajectories with the smallest distance, initially $n/2$  sub-trajectories for a trajectory with $n$ points, until the stopping condition is met.
%
The distances of newly generated line segments are recalculated during the process.
%
These algorithms originally only support \ped, but are easy to be extended to support \sed~\cite{Meratnia:Spatiotemporal}.
%
The batch nature and high time complexities make batch algorithms impractical for online  and/or resource-constrained scenarios \cite{Lin:Operb}.


\stitle{Online algorithms.}
The online algorithms adopt a constrained global distance checking policy that restricts the checking within a sliding or opening window.
%, hence, they are online algorithms.
Constrained global checking algorithms do not need to have the entire trajectory ready before they start compressing, and are more appropriate than batch algorithm for compressing trajectories for online scenarios.
%Moreover, the existing constrained global checking algorithms~\cite{Keogh:online,Meratnia:Spatiotemporal,Muckell:Compression} usually use a sliding or opening window and compress sub-trajectories in the window.

Several \lsa algorithms have been developed, e.g., by combining \dpa or \pavlidis with sliding or opening windows for online processing\cite{Meratnia:Spatiotemporal}. %, Keogh:online
%\cite{Meratnia:Spatiotemporal} is a combination of Top-down and opening window while \cite{Keogh:online} is a combination of Bottom-up and sliding window.
These methods still have a high time and/or space complexity, which significantly hinders their utility in resource-constrained mobile devices \cite{Liu:BQS}.
%
\bqsa \cite{Liu:BQS, Liu:Amnesic} and \squishe\cite{Muckell:Compression} further optimize the opening window algorithms.
%
\bqsa \cite{Liu:BQS, Liu:Amnesic} speeds up the processing by picking out at most eight special points from an open window based on a convex hull, which, however, hardly supports \sed.
%The time complexity of \bqsa is $O(n^2)$ in the worst case.
%it \eat{optimizes the \pavlidis by using}
The \squishe\cite{Muckell:Compression} algorithm is a combination of {opening} window and bottom-up online algorithm. It uses a doubly linked list $Q$ to achieve a better efficiency. Although \squishe supports \sed, it is not one-pass, and has a relatively poor compression ratio.

\stitle{One-pass algorithms.}
The one-pass algorithms adopt a local distance checking policy.
\eat{The local checking policy, the key to achieve the \emph{one-pass} processing, They do not need a window to buffer the preview read points.
Meanwhile, a trajectory compression algorithm is {\em one-pass} if it processes each point in a trajectory once and only once when compressing the trajectory.
}
They do not need a window to buffer the previously read points as they process each point in a trajectory once and only once.
Obviously, the one-pass algorithms run in linear time and constant space.
%

The $n$--${th}$ point routine and the routine of random-selection of points \cite{Shi:Survey} are two naive one-pass algorithms.
In these routines, for every fixed number of consecutive points along the line, the $n$--${th}$  point and one random point among them are retained, respectively.
They are fast, but are obviously not error bounded.
%
In Reumann-Witkam routine\cite{Reumann:Strip}, it builds a strip paralleling to the line connecting the first two points, then the points within this strip compose one section of the line.
The Reumann-Witkam routine also runs fast, but has limited compression ratios.
%
The sector intersection (\cia) algorithm \cite{Williams:Longest, Sklansky:Cone} was developed for graphic and pattern recognition in the late 1970s, for the approximation of arbitrary planar curves by linear segments or finding a polygonal approximation of a set of input data points in a 2D Cartesian coordinate system. \cite{Dunham:Cone} optimized algorithm \cia by considering the distance between a potential end point and the initial point of a line segment, and the \sleeve algorithm \cite{Zhao:Sleeve} in the cartographic discipline essentially applies the same idea as the \cia algorithm.
%
%Moreover, {fast \bqsa \cite{Liu:BQS} (\fbqsa in short), the simplified version of \bqsa, has a linear time complexity.}
%
The authors of this article also developed a One-Pass ERror Bounded (\operb) algorithm \cite{Lin:Operb}.
%
However, all existing one-pass algorithms  use \ped \cite{Williams:Longest, Sklansky:Cone, Dunham:Cone, Zhao:Sleeve,Lin:Operb}, while this study focuses on \sed.

%\textcolor{blue}{
%	There are also semantics based trajectory compression methods that are orthogonal to \lsa based methods (see~\cite{Han:Compress} for more details), and these methods % may be combined with each other to further improve the effectiveness of trajectory compression.}


\subsection{Semantics based methods}
The trajectories of certain moving objects such as cars and trucks are constrained by road networks. These moving objects typically travel along road networks, instead of the line segment between two points. Trajectory compression methods based on road networks \cite{Chen:Trajectory, Popa:Spatio,Civilis:Techniques,Hung:Clustering, Gotsman:Compaction, Song:PRESS, Han:Compress}  project trajectory points onto roads (also known as Map-Matching). Moreover, \cite{Gotsman:Compaction, Song:PRESS, Han:Compress} mine and use high frequency patterns of compressed trajectories, instead of roads, to further improve compression effectiveness.
%
Some methods \cite{Schmid:Semantic, Richter:Semantic} compress trajectories beyond the use of road networks, and further make use
of other user specified domain knowledge, such as places of interests along the trajectories \cite{Richter:Semantic}.
%
%There are also compression algorithms preserving the direction of the trajectory \cite{Long:Direction}. %As it mentions that the trajectory of a moving object contains a large amount of semantic information that could be used to compress the trajectory. For example, the driving direction of a moving object implies the information of the next direction.

These  semantics based approaches are orthogonal to line simplification based methods, and may be combined with each other to  improve the effectiveness of trajectory compression.
