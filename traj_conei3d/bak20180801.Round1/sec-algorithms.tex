%\newdef{definition}{Definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1ex}
\section{Line Simplification Algorithms}
\label{sec-lsa}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{table*}
\vspace{1ex}
\centering
\small
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\kw{Short}  & \kw{Full}    & \kw{Type}  &\multicolumn{2}{|c|}{\kw{Error~Metrics}} &\multicolumn{2}{|c|}{ \kw{Time Complexity}} & \kw{Space} \\
\cline{4-8}
\kw{Name}   & \kw{Name}    &            &\kw{\ped} &\kw{\sed} & \kw{Best}& \kw{Worst}& \kw{Complexity}\\
\hline\hline

DP	&Douglas-Peucker   &batch  &Y &Y   & $\Omega(n)$ & $O(n^2)$ & O(n)  \\
\hline

TP	&\pavlidis   &batch    &Y &Y   & $\Omega(n\log n)$ & $O(n^2/K)$ & O(n)  \\
\hline

BQS	&Bounded Quadrant System &online   &Y   & N &$\Omega(n)$ & $O(n^2)$  & $O(|Q|)$   \\
\hline

\squishe	& {Spatial QUalIty Simplification Heuristic - Extended}   &online    & N &Y   & $\Omega(n\log|Q|)$ & $O(n\log|Q|)$ & $O(|Q|)$ \\
\hline

%swab	&   &online   &Y &Y  & $\Omega(n)$ & $O(n^2)$ & O(w) &\textcolor[rgb]{1.00,0.00,0.00}{} \\
%\hline
%
%owtd	&   &online    &Y &Y   & $\Omega(n)$ & $O(n^2)$ & O(n) &\textcolor[rgb]{1.00,0.00,0.00}{} \\
%\hline
%
%ddr	    &       &one-pass    &Y  &Y  & $\Omega(n)$ & $O(n)$ & O(n) &\textcolor[rgb]{1.00,0.00,0.00}{} \\
%\hline

\operb	& One-pass ERror Bounded  &one-pass   &Y & N &   $\Omega(n)$ & $O(n)$ & O(1)  \\
\hline

\cia	& Cone Intersection      &one-pass   &Y & N &   $\Omega(n)$ & $O(n)$ & O(1) \\
\hline



\end{tabular}
\vspace{-2ex}
\caption{\small Summary of line simplification algorithms. $K$ is the number of the final segments and $|Q|$ is the buffer size.}
\label{tab:summary-lsa}
%\vspace{-3ex}
\end{table*}


This section reviews the six techniques evaluated in our experiments, namely,
(i) Douglas-Peucker\cite{Douglas:Peucker} and \pavlidis~\cite{Pavlidis:Segment}, two distinct \emph{global checking} algorithms.
More specially, we use Douglas-Peucker as the baseline,
(ii) \bqsa\cite{Liu:BQS} and \squishe~\cite{Muckell:SQUISH}, the famous \emph{constrained global checking} methods for trajectories, %\textcolor[rgb]{1.00,0.00,0.00}{\opwa, \swab  and}
and (iii) \operb\cite{Lin:Operb} and Cone-Intersection\cite{Williams:Longest,Sklansky:Cone,Dunham:Cone, Zhao:Sleeve}, the \emph{local checking} algorithms.
Table~\ref{tab:summary-lsa} is a summary of these algorithms.
Interested readers are referred to Appendix A for a summary of other existing work on trajectory compression.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Douglas-Peucker Algorithm}

The Douglas-Peucker algorithm \cite{Douglas:Peucker} was invented in 1973, for reducing the number of points required to represent a digitized line or its caricature in the context of computer graphics and image processing.
The Douglas-Peucker algorithm employ the Top-down and the global checking policy and it is clearly a batch algorithm, hence, the entire raw trajectory is needed at the beginning \cite{Meratnia:Spatiotemporal}.

The basic Douglas-Peucker algorithm (\dpa) is shown in Figure~\ref{alg:dp}.
Given a trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_n]$ and an error bound $\epsilon$, the algorithm uses the first point $P_0$ and the last point $P_n$ of \trajec{T} as the start point $P_s$ and the end point $P_e$ of the first line segment $\mathcal{L}(P_0, P_n)$, then it calculates the distance $ped(P_i, {\mathcal{L}})$ for each point $P_i$ ($i\in[0,n]$) (lines 1--2). If $ped(P_k, {\mathcal{L}})$ = $\max \{ped(P_0, {\mathcal{L}}), \ldots, ped(P_n, {\mathcal{L}}) \} \le \epsilon$, then it returns $\{\mathcal{L}(P_0,P_n)\}$ (lines 3--5). Otherwise, it divides \trajec{T} into two sub-trajectories \trajec{T}$[P_0, \ldots, P_k]$ and \trajec{T}$[P_{k}, \ldots, P_n]$, and recursively compresses these sub-trajectories until the entire trajectory has been considered (lines 6--7).
Moreover, \cite{Meratnia:Spatiotemporal} revised the \dpa algorithm to support \sed.
The \sed enabled \dpa runs in the same routine as the basic \dpa, except that $ped$ is replace by $sed$.

In the best case, the time complexity of \dpa is $\Omega(n)$; in the worst case, the time complexity of \dpa is $O(n^2)$.
The \dpa algorithm are \textcolor[rgb]{0.00,0.07,1.00}{widely considered have excellent compression ratio and accuracy.}
However, the batch nature and high time complexity make it not suitable for the online scenarios.


%%%%%%%%%%%%%%%%%%%%%Baseline Algorithm
\begin{figure}[tb!]
%\vspace{-2ex}
\begin{center}
{\small
\begin{minipage}{3.36in}
\myhrule \vspace{-1ex}
\mat{0ex}{
{\bf Algorithm}~$\dpa(\dddot{\mathcal{T}}[P_0,\ldots,P_n], \epsilon)$\\
\sstab
\bcc \hspace{1ex}\=\For each point $P_i$ ($i\in[0,n]$) in $\dddot{\mathcal{T}}[P_0, \ldots, P_n]$ \Do\\
\icc \>\hspace{3ex}compute $ped(P_i, {\mathcal{L}})$ between $P_i$ and ${\mathcal{L}}(P_0, P_n)$;\\
\icc \> \Let $ped(P_k, {\mathcal{L}})$ := $\max \{ped(P_0, {\mathcal{L}}), \ldots, ped(P_n, {\mathcal{L}}) \}$;\\
\icc \> \If $ped(P_k, {\mathcal{L}}) \le\epsilon$ \Then \\
\icc \> \hspace{3ex}\Return $\{\mathcal{L}(P_0,P_n)\}$.\\
\icc \> \Else\\
\icc \> \hspace{3ex}\Return $\dpa($\trajec{T}$[P_0, \ldots, P_k], \epsilon)\cup\dpa($\trajec{T}$[P_{k}, \ldots, P_n], \epsilon)$.
}
\vspace{-2.5ex}
\myhrule
\end{minipage}
}
\end{center}
\vspace{-3ex}
\caption{\small Basic Douglas-Peucker algorithm}
\label{alg:dp}
\vspace{-2ex}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%\vspace{-1ex}
\begin{example}
\label{exm-alg-lsa}
Consider the trajectory $\dddot{\mathcal{T}}[P_0,\ldots,P_{8}]$ shown in Figure~\ref{fig:notations}.
The $\dpa$ Algorithm firstly creates $\vv{P_0P_{8}}$, then it calculates the distance of each point in $\{P_0,\ldots,P_{8}\}$ to $\vv{P_0P_{8}}$.
It finds that $P_{4}$ has the maximum distance to $\vv{P_0P_{8}}$, which is greater than $\epsilon$. Then it goes to compress sub-trajectories $[P_0, \ldots, P_{4}]$ and $[P_{4}, \ldots, P_{8}]$, separately.
When using \sed (right), the sub-trajectory $[P_4,\ldots, P_{8}]$ is further split to $[P_4$, $\ldots$, $P_7]$ and $[P_7$, $P_{8}]$.
Finally, the algorithm outputs two continuous directed line segments $\vv{P_0P_4}$ and $\vv{P_4P_8}$ when using \ped, and three continuous directed line segments $\vv{P_0P_4}$, $\vv{P_4P_7}$ and $\vv{P_7P_8}$ when using \sed.
\end{example}




\subsection{Theo Pavlidis' Algorithm}

\begin{figure*}[tb!]
\centering
\includegraphics[scale=0.66]{figures/Fig-Pavlidis.png}
\vspace{-1ex}
\caption{\small The trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_{8}]$ is compressed by the \pavlidis algorithm using \ped. The triple $(i, j, cost)$ is the $cost$ of merging the line segments $\overline{P_iP_t}$ and $\overline{P_tP_j}$. }
\vspace{-2ex}
\label{fig:pavlidis}
\end{figure*}

The {\pavlidis algorithm}~\cite{Pavlidis:Segment}, invented in 1974, is known as the Bottom-up algorithm. It also employs the global checking policy, and hence, it is a batch algorithm. Note that the original \pavlidis algorithm outputs disjoint line segments. We slightly modify the algorithm to output the continuous line segments.

Given a trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_n]$ and an error bound $\epsilon$,
the algorithm begins by creating the finest possible approximation of the trajectory, \ie $[P_0, P_1], [P_2, P_3], \ldots$, so that $n-1$ segments are used to approximate the original $n$-length trajectory.
Next, for each pair of adjacent segments $[P_{s}, P_{s+1}]$ and $[P_{s+1}, P_{s+2}]$, $0\le s \le n-2$,
the max distance of each points $P_i$, $0<i<2$, to the line segment $\vv{P_sP_{s+2}}$, \ie $ped(P_{s+i}, \vv{P_sP_{s+2}})$, is calculated and the maximum of them is saved, denoting the \emph{cost} of merging them.
Then the algorithm begins to iteratively merge the lowest cost adjacent segments pair
until that no cost is below the error bound $\epsilon$.
When the pair of adjacent segments $[P_{s}, P_{s+i}]$ and $[P_{s+i}, P_{s+j}]$ are merged to a new segment $[P_{s}, P_{s+j}]$, the algorithm needs to recalculate the cost of merging the new segment with its preceding neighbour segment, and the cost of merging it with its successor neighbor segment.
The time complexity of the algorithm is $O(n^2/K)$, where $K$ is the number of the final segments.
Furthermore, it is also easy to implement \sed in the \pavlidis algorithm.

%\textcolor[rgb]{1.00,0.00,0.00}{Todo...dis-continuous line segments.}

\begin{example}
\label{exm-alg-squishe}
Figure~\ref{fig:pavlidis} is a running example of the \pavlidis algorithm.
(1) Initially, the $10$ segments are created, and for each pair of adjacent segments, the costs of merging them are calculated and saved. For example, the cost of merging $\vv{P_0P_1}$ and $\vv{P_1P_2}$ is $ped(P_{1}, \vv{P_0P_{2}}) = 0.32\epsilon$.
(2) The cost of merging $\vv{P_6P_7}$ and $\vv{P_7P_8}$ is $0.02\epsilon$, which is the minimal value among all costs. Hence, $\vv{P_6P_7}$ and $\vv{P_7P_8}$ are merged to $\vv{P_6P_8}$. The cost of merging $\vv{P_5P_6}$ and $\vv{P_6P_8}$, and the cost of merging $\vv{P_6P_8}$ and $\vv{P_8P_9}$ are also updated to $0.37\epsilon$ and $0.11\epsilon$, respectively.
(3) $\vv{P_2P_3}$ and $\vv{P_3P_4}$ are merged to $\vv{P_2P_4}$. The cost merging $\vv{P_2P_4}$ and $\vv{P_4P_5}$, and the cost of merging $\vv{P_1P_2}$ and $\vv{P_2P_4}$ are also updated.
(4) At last, the algorithm outputs two line segments $\vv{P_0P_4}$ and $\vv{P_4P_{10}}$.
\end{example}






\begin{figure}[tb!]
\centering
\includegraphics[scale = 0.66]{figures/Fig-BQS.png}
\vspace{-2ex}
\caption{{\small Examples for algorithm \bqsa.}}
\label{fig:bqs}
\vspace{-3ex}
\end{figure}


\begin{figure*}[tb!]
\centering
\includegraphics[scale=0.66]{figures/Fig-Squishe.png}
\vspace{-1ex}
\caption{\small The trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_{10}]$ is compressed by the \squishe algorithm using \sed to five line segments. The size of Q is 6, and the data structure after point $P$ is a tuple $(pred, succ, mmprio, prio)$. }
\vspace{-2ex}
\label{fig:squishe}
\end{figure*}

\subsection{Bounded Quadrant System}
The \bqsa algorithm \cite{Liu:BQS} is essentially an opening window and top-down algorithm\cite{Meratnia:Spatiotemporal}. It further reduces the compression time by introducing a convex hull that bounds a certain number of points. For a buffered sub trajectory $[P_s, \ldots, P_k]$, it splits the space into four quadrants. For each quadrant, a rectangular bounding box is firstly created using the least and highest $x$ values and the least and highest $y$ values among points $P_s,\ldots,P_k$. Then another two bounding lines connecting points $P_s$ and $P_{h}$ and points $P_s$ and $P_{l}$ are created such that lines $\vv{P_sP_{h}}$ and $\vv{P_sP_{l}}$ have the largest and smallest angles with the $x$-axis, respectively.
Here $P_{h},P_{l} \in\{P_s,\ldots,P_k\}$. The bounding box and the two lines together form a convex hull.
A buffer here is similar to the window in algorithm \opwa.
Each time a new point is added to the buffer, \bqsa first picks out at most eight significant points in a quadrant. It calculates the distances of the significant points to line $\vv{P_sP_e}$, among which the largest distance $d_{u}$ and the smallest distance $d_l$ are an upper bound and  a lower bound of the distances of all points in $[P_s, \ldots, P_k]$ to line $\vv{P_sP_e}$.
(1) If $d_l\ge \epsilon$, it produces a new line segment $\mathcal{L}(P_{s}, P_{k})$, and replaces $W$ with a new window $[P_{k},\ldots,P_{k+1}]$.
(2) If $d_u < \epsilon$, it simply expands the buffer to $[P_s, \ldots, P_k, P_{k+1}]$ $(k+1\le n)$ by adding a new point $P_{k+1}$.
(3) Otherwise, it computes all distances $d(P_i, {\mathcal{L}(P_s,P_k)})$ ($i\in[s, k]$) as algorithm \dpa.
%It does the same as (2) if the largest distances is less than $\epsilon$, and does the same as (1), otherwise.
The time complexity of \bqsa remains $O(n^2)$.
Its simplified version \fbqsa essentially avoids case (3) to achieve an $O(n)$ time complexity.
Note \bqsa and \fbqsa only support \ped.

\begin{example}
\label{exm-alg-bqs}
In Figure~\ref{fig:bqs}, the bounding box $c_1c_2c_3c_4$ and the two lines $\vv{P_sP_{h}} = \vv{P_0P_1}$ and $\vv{P_sP_{l}} = \vv{P_0P_2}$ form a convex hull $u_1u_2c_2l_2l_1c_4$. \bqsa computes the distances of $u_1,u_2,c_2,l_2,l_1$ and $c_4$ to line $\vv{P_0P_6}$ when $k=6$ or to line $\vv{P_0P_7}$ when $k=7$.

When $k=6$, all these distances to $\vv{P_0P_6}$  are less than $\epsilon$, hence \bqsa goes on to the next point (case 1); When $k=7$,
the max and min distances to $\vv{P_0P_7}$ are larger and less than $\epsilon$, respectively, and \bqsa needs to compress sub-trajectory $[P_0, \ldots, P_7]$ along the same line as \dpa (case 2).
\end{example}




%However, these existing online algorithms are not one-pass.  In this study, we propose a novel local distance checking method, based on which we develop one-pass online algorithms that are totally different from the window based algorithms. Further, as shown in the experimental study, our approaches are clearly superior to the existing online algorithms, in terms of both efficiency and effectiveness.

\subsection{SQUISH-E}



The \squishe algorithm~\cite{Muckell:Compression} is an {opening} window and Bottom-up algorithm, taking as input a trajectory \trajec{T} and two additional parameters $\lambda$ and $\epsilon$.
It compresses trajectory \trajec{T} while striving to minimize \sed error and achieving the compression ratio of $\lambda$. Then, it further compresses \trajec{T} as long as this compression will not increase the max \sed error beyond $\epsilon$.

\eat{%%%%%%%%%%%%%%%%
Meanwhile, \squishe($\lambda$) is the case where $\epsilon$ is set to $0$ and therefore it minimizes \sed error ensuring the compression ratio of $\lambda$, and
\squishe($\epsilon$) denotes another case, \ie the \emph{min-$\#$ problem}, where $\lambda$ is set to $1$ and therefore it maximizes compression ratio while keeping \sed error under $\epsilon$.
In this paper, we only discuss \squishe($\epsilon$).
}%%%%%%%%%%%%%%%%%%%%

The \squishe algorithm optimizes the \pavlidis by using a doubly linked list $Q$. Each node in the list is a tuple, \ie $P(pred, succ, mnprio, prio)$, where $P$ is a trajectory data point (for convenience, we also refer $P$ to the node), $pred$ refers to its predecessor node/point, $succ$ refers to its successor node/point, $prio$ is the priority of the point, which is defined as an upper bound on the \sed error that the removal of that point would introduce, and $mnprio$ is the max priority of its predecessor and successor points that have ever been removed from the list.
%
Initially, trajectory points are loaded to the list $Q$ one by one.
At the same time, $mnprio$ of each point is set to $0$ as no node has been removed from the list.
Moreover, the priorities of points $P_0$ and $P_{|Q|-1}$ are set to $\infty$, and the priority of point $P_i$, $0<i<|Q|-1$, is set to $sed(P_i, \vv{pred(P_{i})succ(P_{i})})$.
%
Then, \squishe finds and removes a point $P_j$ from $Q$ that has the lowest priority and $prio(P_j)<\epsilon$.
And, the properties $mnprio$ of predecessor point $pred(P_j)$ and successor point $succ(P_j)$ are updated to $max\{mnprio(pred(P_j)), ~prio(P_j)\}$ and $max\{mnprio(succ(P_j)), ~prio(P_j)\}$ , respectively.
Next, the properties $prio$ of points $pred(P_j)$ and $succ(P_j)$ are further updated to $mnprio(pred(P_j)) + sed(pred(P_j), \vv{pred(pred(P_{j}))succ(P_{j})})$ and $mnprio(succ(P_j)) + sed(succ(P_j),\vv{pred(P_{j})succ(succ(P_{j}))})$, respectively.
%
After that, a new point is read to the list and the information of its predecessor point in the list is updated.
%
The above process repeated until that no point has the priority smaller than $\epsilon$. % \ie  the \sed up bound

\squishe finds and removes a point from $Q$ that has the lowest priority in $O(\log |Q|)$ time, where $|Q|$ denotes the number of points stored in $Q$.
Thus, \squishe runs in $O(n\log |Q|)$ time and $O(|Q|)$ space. Note \squishe does not support \ped.

\begin{example}
\label{exm-alg-squishe}
Figure~\ref{fig:squishe} is a running example of \squishe.
(1) Initially, $|Q| = 6$ points are read to the list. The tuple $(pred, succ, mmprio, prio)$ for each point is initialized. For example, the tuple of $P_1$ is set to $(0, 2, 0, 0.42\epsilon)$, where $0.42\epsilon$ is the \sed from $P_1$ to $\vv{P_0P_2}$.
(2) The priority of $P_3$ has the minimal value, thus, it is removed from the list.
The $mnprio$ properties of $P_2$ and $P_4$ are updated to $max\{mnprio(pred(P_3)), prio(P_3)\}$ = $max\{mnprio(P_2), prio(P_3)\}$ = $max\{0, 0.39\epsilon\}$ = $0.39\epsilon$, and $max\{mnprio(P_4), ~prio(P_3)\}$ = $0.39\epsilon$, respectively.
Furthermore, the $prio$ property of $P_4$ is updated to $mnprio(succ(P_j)) + sed(succ(P_j),\vv{pred(P_{j})succ(succ(P_{j}))})$ = $mnprio(P_4) + sed(P_4,\vv{P_2P_5})$ = $0.39\epsilon + 2.50\epsilon$ = $2.89\epsilon$, and the $prio$ property of $P_2$ is updated to $mnprio(P_2) + sed(P_2,\vv{P_1P_4})$ = $0.39\epsilon + 2.12\epsilon$ = $2.51\epsilon$.
Then, $P_6$ is read, and the information of $P_5$ is updated.
(3) $P_5$ is removed and $P_7$ is read to the list.
(4) Finally, the algorithm outputs 5 line segments $\vv{P_0P_2},\vv{P_2P_4},\vv{P_4P_7},\vv{P_7P_9}$ and $\vv{P_9P_{10}}$.
\end{example}






\eat{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\subsection{Opening Window and Top-down}}

The \opwa algorithm~\cite{Meratnia:Spatiotemporal} combines the Top-down and opening window strategies, and enforces the constrained global checking in the window.

Given a trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_n]$ and an error bound $\epsilon$, algorithm \opwa~\cite{Meratnia:Spatiotemporal} maintains a window $W[P_s, \ldots, P_k]$, where $P_s$ and $P_k$ are the start and end points, respectively. Initially, $P_s$ = $P_0$ and $P_k$ = $P_1$, and the window $W$ is gradually expanded by adding new points one by one. \opwa tries to compress all points in $W[P_s, \ldots, P_k]$ to a single line segment $\mathcal{L}(P_{s}, P_{k})$. If the distances $ped(P_i, {\mathcal{L}})\le \epsilon$ for all points $P_i$ ($i\in[s, k]$), it simply expands $W$ to $[P_s, \ldots, P_k, P_{k+1}]$ $(k+1\le n)$ by adding a new point $P_{k+1}$. Otherwise, it produces a new line segment $\mathcal{L}(P_{s}, P_{k-1})$, and replaces $W$ with a new window $[P_{k-1},\ldots,P_{k+1}]$. The above process repeats until all points in $\dddot{\mathcal{T}}$ have been considered.

%\textcolor[rgb]{0.00,0.07,1.00}{According to the different methods of selecting the end points of a line segment, Open Window can further be divided into Normal Penning Window and Before Opening Window~\cite{Meratnia:Spatiotemporal}. When the distance of the point to compressed trajectory exceeds a certain threshold, Normal Opening Window algorithm select that point as the end point, while Before Opening Window select the last point within the window as the end point of the current trajectory.}

Algorithm \opwa is not efficient enough for compressing long trajectories as it remains in $O(n^2)$ time, the same as the \dpa algorithm.
Also, \ped and \sed are both supported in \opwa as the algorithm \dpa does.


\begin{example}
\label{exm-alg-opw}
\end{example}


\textcolor[rgb]{1.00,0.00,0.00}{\subsection{Sliding Window and Bottom-up}}

The \swab algorithm~\cite{Keogh:online} is essentially the combination of the Sliding Window mechanism and the Bottom-up algorithm.
It keeps a window, $w[P_s, \ldots, P_{s+k-1}]$, of a fixed size of $k$.
The window size $k$ should be carefully chosen so that there are enough data points in the window to create about 5 or 6 line segments \cite{Keogh:online}.
Initially, $P_s=P_0$.
Next, the Bottom-Up algorithm, \eg \pavlidis algorithm, is applied to the points in the window, which merges the points into segments with the left-most segment being $\vv{P_sP_{s+i}}$, $i<k$.
Then $\vv{P_sP_{s+i}}$ is output, the window slides to right taking $P_{s+i+1}$ as the new start point of the window, and the Bottom-Up algorithm is applied again.
This process repeated until all points have been merged to segments.

The time complexity of \swab is a small constant factor worse than that of the standard Bottom-Up algorithm~\cite{Keogh:online}.
Also, it supports \sed. % as the standard Bottom-Up algorithm does.

\textcolor[rgb]{1.00,0.00,0.00}{Todo...dis-continuous line segments.}






\subsection{Reumann-Witkam and LDR}

\begin{figure*}[tb!]
\centering
\includegraphics[scale=0.66]{figures/Fig-LDR.png}
\vspace{-1ex}
\caption{\small The trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_{10}]$ is compressed by the Reumann-Witkam and Linear Dead Reckoning algorithms to four and eight line segments, respectively.}
\vspace{-2ex}
\label{fig:ldr}
\end{figure*}

In Reumann-Witkam\cite{Reumann:Strip}, the input data is divided into sections by strips.
Initially, the first strip, with the width of $2*\epsilon$, takes the line $\vv{P_0P_1}$ connecting the first two points, $P_0$ and $P_1$ as its middle line.
Then the strip is expending over the line into the direction of its initial tangent, covering the succeed points, $P_2, \ldots, P_{j}$, until the strip hits the line $\vv{P_jP_{j+1}}$ (meaning that the next point $P_{j+1}$, $j>1$, is out side of the strip).
The points, $[P_0, \ldots, P_{j}]$, within this strip compose a section. The first and last points of the section, \ie $P_0,P_{j}$, are output, and those points between them are removed.
The last point $P_{j}$ is the initial point of the next strip.
The whole process is repeated until the strip contains the end point $P_n$ of the input data.
The Reumann-Witkam is a one-pass algorithm.

{The Linear Dead Reckoning (LDR)\cite{Lange:Tracking} for position tracking follows the similar routine as the Reumann-Witkam algorithm except that it assumes a velocity ${\vv{v}}$ for each section and uses \sed instead of \ped in distance checking.
Moreover, the authors of \cite{Trajcevski:DDR} proved that LDR is also suitable for online spatio-temporal compression as long as the tolerance threshold of the algorithm is set to $\epsilon/2$.}

\begin{example}
\label{exm-alg-strip}
In Figure~\ref{fig:ldr}, the trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_{10}]$ is compressed
%
(1) by the Reumann-Witkam to four line segments $\vv{P_0P_2}$, $\vv{P_2P_4}$, $\vv{P_4P_7}$ and $\vv{P_7P_{10}}$. First, a strip with width $2\epsilon$ is built parallel to the line $\vv{P_0P_1}$, then the strip is extended over the line and includes point $P_2$. Because $P_3$ is outside of the strip, $P_2$ becomes the end point of the first section and the start point of the second section.
%
(2) by the Linear Dead Reckoning algorithm to eight line segments $\vv{P_0P_1}$, $\vv{P_1P_2}$, $\vv{P_2P_3}$, $\vv{P_3P_4}$, $\vv{P_4P_5}$, $\vv{P_5P_7}$, $\vv{P_7P_8}$ and $\vv{P_8P_{10}}$. First, an initial velocity ${\vv{v}_0}$ is set to $|P_0P_1|/(t_1-t_0)$. Then the synchronized point $P'_2$ of $P_2$ is estimated based on the velocity ${\vv{v}_0}$ and time of $P_2$, \ie ${v}_0 * (t_2-t_0)$. Because the \sed from $P_2$ to the line $\vv{P_0P'_2}$ , \ie $|P_2P'_2|$, is great than $\epsilon/2$, the algorithm outputs $\vv{P_0P_1}$ and starts the next section.
\end{example}

}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%End of Eat

\subsection{\operb}
\operb\cite{Lin:Operb} is essentially the angle-adjustable strip\cite{Reumann:Strip}.
%Algorithm \operb, after initializing, repeatedly processes the data points in $\dddot{\mathcal{T}}[P_0,$ $\ldots, P_{n}]$ one by one until that all data points have been considered.
Consider an error bound $\epsilon$ and a sub-trajectory $\dddot{\mathcal{T}_s}[P_s,$ $\ldots, P_{s+k}]$.
\operb dynamically maintains a directed line segment $\mathcal{L}_i$ ($i\in[1,k]$), whose start point is fixed with $P_s$ and its end point is identified (may not in $\{P_s, \ldots, P_{s+i}\}$) to {\em fit} all the previously processed points $\{P_s, \ldots, P_{s+i}\}$.
The directed line segment $\mathcal{L}_i$ is built by a function named \emph{fitting function $\mathbb{F}$}, such that when a new point $P_{s+i+1}$ is considered, only its distance to the directed line segment $\mathcal{L}_i$ is checked, instead of checking the distances of all or a subset of data points of $\{P_{s}, \ldots, P_{s+i}\}$ to $\mathcal{R}_{i+1}$ = $\vv{P_sP_{s+i+1}}$ as the global distance checking does.
During processing, if the distance of point $P_{s+i}$ to the directed line segment $\mathcal{L}_{i-1}$ is larger than the threshold, then a directed line segment, start from $P_s$, is generated and output;
otherwise, the directed line segment $\mathcal{L}_i$ is updated by the fitting function $\mathbb{F}$, as follows.

\begin{small}
\vspace{-2ex}
\begin{equation*}
\label{equ-function}
\hspace{-1.5ex}\left\{
    \begin{aligned}
        &\hspace{-1.5ex}\left[
            \begin{aligned}
           % & |\mathcal{L}_{i}| = |\mathcal{R}_{i-1}|    \\
           % & \mathcal{L}_{i}.\theta = \mathcal{R}_{i-1}.\theta\\
            & \mathcal{L}_{i} = \mathcal{L}_{i-1}\\
            \end{aligned}
        \right]\hspace{12.5ex}~when~(|\mathcal{R}_{i}| - |\mathcal{L}_{i-1}|) \le \frac{\epsilon}{4}   \\
        &\hspace{-1.5ex}\left[
            \begin{aligned}
            & |\mathcal{L}_{i}|  = j*{\epsilon}/{2} \\
            & \mathcal{L}_{i}.\theta = \mathcal{R}_{i}.\theta    \\
            \end{aligned}
        \right]\hspace{8.5ex}~when~|\mathcal{R}_{i}| >  \frac{\epsilon}{4}~\And~|\mathcal{L}_{i-1}|=0    \\
        &\hspace{-1.5ex}\left[
            \begin{aligned}
            & |\mathcal{L}_{i}|  = j*{\epsilon}/{2}\\
            & \mathcal{L}_{i}.\theta = \mathcal{L}_{i-1}.\theta + f(\mathcal{R}_i,\mathcal{L}_{i-1})*\arcsin(\frac{ped(P_{s+i}, \mathcal{L}_{i-1})}{j*\epsilon/2})/j \\	
           % & \theta^- = \mathcal{L}_{i-1}.\theta - \arcsin(\frac{d(P_i, \mathcal{L}_{i-1})}{j*\epsilon/2})/j \\	
           % & \mathcal{L}_{i}.\theta = \arg_{\mathcal{L}_{i}.\theta}\min({d(P_{i+1}, \mathcal{L}_{i}}), \mathcal{L}_{i}.\theta \in\{\theta^+,\theta^-\})\\	
            \end{aligned}
        \right]\hspace{0ex}else\\
    \end{aligned}
    \right.
\end{equation*}
\vspace{-2ex}
\end{small}


\ni where (a) $1 \le i \le k+1$; (b) $\mathcal{R}_{i-1}$ = $\vv{P_sP_{s+i-1}}$, is the directed line segment whose end point $P_{s+i-1}$ is in $\dddot{\mathcal{T}_s}[P_s, \ldots, P_{s+k}]$; (c) $\mathcal{L}_{i}$ is the directed line segment built by fitting function $\mathbb{F}$ to fit sub-trajectory $\dddot{\mathcal{T}_s}[P_s, \ldots, P_{s+i}]$ and $\mathcal{L}_{0}$ = $\mathcal{R}_{0}$; (d) $j = \lceil(|\mathcal{R}_{i}|*2/\epsilon - 0.5)\rceil$; (e) $f()$ is a sign function such that $ f(\mathcal{R}_i,\mathcal{L}_{i-1})$ = $1$ if the included angle $\angle(\mathcal{R}_{i-1}, \mathcal{R}_{i})$ = $(\mathcal{R}_i.\theta - \mathcal{L}_{i-1}.\theta)$ falls in the range of $(-2\pi, -\frac{3\pi}{2}]$, $[-\pi, -\frac{\pi}{2}]$, $[0, \frac{\pi}{2}]$ and $[\pi, \frac{3\pi}{2})$, and $f(\mathcal{R}_i,\mathcal{L}_{i-1})$ = $-1$, otherwise; (f) $\epsilon/2$ is a step length to control the increment of $|\mathcal{L}|$.

Optimization techniques are equipped to achieve a better compression ratio\cite{Lin:Operb}.
Algorithm \operb runs in $O(n)$ time and takes $O(1)$ space.
Note \operb only supports \ped.


\begin{figure}[tb!]
\centering
\includegraphics[scale=0.66]{figures/Fig-oper.png}
\vspace{-1ex}
\caption{\small The trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_{10}]$ is compressed by the \operb algorithm to two line segments.}
\vspace{-2ex}
\label{fig:operb}
\end{figure}


\begin{example}
\label{exm-alg-operb}
Figure~\ref{fig:operb} is a running example of the \operb algorithm.
(1) It takes $P_0$ as the start point, reads $P_1$ and sets $\mathcal{L}_1$ = $\vv{P_0P_1}$.
(2) It reads $P_2$. The distance from $P_2$ to $\mathcal{L}_1$ is less than the threshold, thus, it updates $\mathcal{L}_1$  to $\mathcal{L}_2$ by the fitting function $\mathbb{F}$.
(3) It reads $P_3$ and $P_4$, and updates $\mathcal{L}_2$ to $\mathcal{L}_3$ and $\mathcal{L}_3$ to $\mathcal{L}_4$, respectively.
(4) It reads $P_5$. The distance from $P_5$ to $\mathcal{L}_4$ is larger than the threshold, thus, it outputs $\vv{P_0P_4}$ and start the next section taking $P_4$ as the new start point.
(5) The process continues until all points have been processed. At last, the algorithm outputs two continuous line segments $\vv{P_0P_4}$ and $\vv{P_4P_{10}}$.
\end{example}



\subsection {Cone Intersection}

The Cone Intersection (\cia) algorithms \cite{Williams:Longest,Sklansky:Cone,Dunham:Cone}, also named \sleeve in \cite{Zhao:Sleeve}, are {the angle-adjustable strips too}.
%Note that the ``cone intersection" algorithms, developed in fields of graphic, cartographic and pattern recognition, are still not familiar to researchers of trajectory compression.
Given a sequence of points $[P_{s}, P_{s+1}, \ldots, P_{s+k}]$ and an error bound $\epsilon$, the \cia approach processes the input points in order and produces a simplified poly-line.  Moreover, instead of using the distance tolerance $\epsilon$ directly as the distance threshold, \cia converts the distance tolerance into a variable angle tolerance for testing the points.
%
For the start point $P_s$ and any point $P_{s+i}$, $1\le i\le k$, there are two different lines $\vv{P_sP^u_{s+i}}$ and $\vv{P_sP^l_{s+i}}$, satisfying $ped(P_i, \vv{P_sP^u_{s+i}}) = \epsilon$, $ped(P_i, \vv{P_sP^l_{s+i}}) = \epsilon$ and $\vv{P_sP^l_{s+i}}.\theta < \vv{P_sP^u_{s+i}}.\theta$. Indeed, they forms a \emph{cone} $\mathcal{C}$($P_s$, $P_{s+i}$, $\epsilon$) taking $P_s$ as the center point and $\vv{P_sP^u_{s+i}}$ and $\vv{P_sP^l_{s+i}}$ as the border lines.
% , where $P_s$ is the center point, and $\vv{P_sP^l_{s+i}}$ and $\vv{P_sP^l_{s+i}}$ are two bound lines.
Then there exists a point $Q$ such that all points $P_{s+i}$, $i \in [1, ... k]$, have perpendicular distances to
line $\mathcal{L}(P_s,Q)$ within the given \ped tolerance $\epsilon$ if and only if $\bigcap_{i=1}^{k}\mathcal{C}(P_s, P_{s+i}, \epsilon) \ne \phi$.
Note that in this case, point $Q$ may not belong to $[P_{s}, P_{s+1}, \ldots, P_{s+k}]$.
However, if the last point $P_{s+k}$ is chosen as the end point $Q$ of the approximating line segment, then a narrow \emph{cone} with a distance tolerance $\epsilon/2$ could be employed to ensure the algorithm be \ped error bounded by $\epsilon$\cite{Zhao:Sleeve}.

Algorithm \cia runs in $O(n)$ time and takes $O(1)$ space.
\cia also only supports \ped.


\begin{figure}[tb!]
\centering
\includegraphics[scale=0.66]{figures/Fig-sleeve.png}
\vspace{-1ex}
\caption{\small The trajectory $\dddot{\mathcal{T}}[P_0, \ldots, P_{10}]$ is compressed by the \conei algorithm to two line segments.}
\vspace{-2ex}
\label{fig:sleeve}
\end{figure}


\begin{example}
\label{exm-alg-sleeve}
Figure~\ref{fig:sleeve} is a running example of narrow \emph{cone} taking a distance tolerance $\epsilon/2$. At the beginning, $P_0$ is the first start point, $P_1$, $P_2$, $P_3$ and so on, each has a narrow \emph{cone}.
For example, the narrow \emph{cone} $\mathcal{C}$($P_0$, $P_{3}$, $\epsilon/2$) takes $P_0$ as the center point and $\vv{P_0P^u_{3}}$ and $\vv{P_0P^l_{3}}$ as the border lines.
Because $\bigcap_{i=1}^{4}\mathcal{C}(P_0, P_{s+i}, \epsilon/2) \ne \phi$ and $\bigcap_{i=1}^{5}\mathcal{C}(P_0, P_{s+i}, \epsilon/2) = \phi$, $\vv{P_0P_4}$ is output and $P_4$ becomes the start point of the next section.
At last, the algorithm outputs two continuous line segments $\vv{P_0P_4}$ and $\vv{P_4P_{10}}$.
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



